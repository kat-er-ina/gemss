{
  "tiers": {
    "tier1": {
      "name": "Comprehensive Small-Scale Validation",
      "description": "Establishes baseline performance metrics on **clean data** (NOISE_STD=0.1, NAN_RATIO=0.0). Focuses on core scenarios where the number of features (\u226a 100) significantly outweighs the number of samples (n \u226a p), as well as near-square scenarios (n \u2248 p), using the primary sparsity levels (k=3, 5, 10). This tier provides the fundamental stability and recovery validation.",
      "algorithm_parameters": {
        "N_ITER": 4000,
        "PRIOR_TYPE": "sss",
        "STUDENT_DF": 1,
        "STUDENT_SCALE": 1.0,
        "VAR_SLAB": 100.0,
        "VAR_SPIKE": 0.1,
        "WEIGHT_SLAB": 0.9,
        "WEIGHT_SPIKE": 0.1,
        "IS_REGULARIZED": true,
        "LEARNING_RATE": 0.002,
        "MIN_MU_THRESHOLD": 0.2,
        "BINARIZE": true
      },
      "combinations": [
        "20,100,3,3,0.1,0.0,6,500,16,0.5",
        "20,100,3,5,0.1,0.0,6,500,16,0.5",
        "20,200,3,3,0.1,0.0,6,500,16,0.5",
        "20,200,3,5,0.1,0.0,6,500,16,0.5",
        "20,500,3,3,0.1,0.0,6,500,16,0.5",
        "20,500,3,5,0.1,0.0,6,500,16,0.5",
        "50,100,3,3,0.1,0.0,6,500,16,0.5",
        "50,100,3,5,0.1,0.0,6,500,16,0.5",
        "50,100,3,10,0.1,0.0,6,500,16,0.5",
        "50,200,3,3,0.1,0.0,6,500,16,0.5",
        "50,200,3,5,0.1,0.0,6,500,16,0.5",
        "50,200,3,10,0.1,0.0,6,500,16,0.5",
        "50,500,3,3,0.1,0.0,6,500,16,0.5",
        "50,500,3,5,0.1,0.0,6,500,16,0.5",
        "50,500,3,10,0.1,0.0,6,500,16,0.5",
        "100,100,3,3,0.1,0.0,6,500,16,0.5",
        "100,100,3,5,0.1,0.0,6,500,16,0.5",
        "100,100,3,10,0.1,0.0,6,500,16,0.5",
        "100,200,3,3,0.1,0.0,6,500,16,0.5",
        "100,200,3,5,0.1,0.0,6,500,16,0.5",
        "100,200,3,10,0.1,0.0,6,500,16,0.5",
        "100,500,3,3,0.1,0.0,6,500,16,0.5",
        "100,500,3,5,0.1,0.0,6,500,16,0.5",
        "100,500,3,10,0.1,0.0,6,500,16,0.5"
      ]
    },
    "tier2": {
      "name": "High-Dimensional Stress Test (Pushed limits)",
      "description": "Tests the algorithm's **scalability and signal detection capability in extreme high-dimensional contexts** (p \u2265 1000). Focuses on the core n \u226a p regime near the imposed p=2000 feature limit and explores the boundary case of p=5000, simulating highly feature-rich biomedical datasets.",
      "algorithm_parameters": {
        "N_ITER": 4000,
        "PRIOR_TYPE": "sss",
        "STUDENT_DF": 1,
        "STUDENT_SCALE": 1.0,
        "VAR_SLAB": 100.0,
        "VAR_SPIKE": 0.1,
        "WEIGHT_SLAB": 0.9,
        "WEIGHT_SPIKE": 0.1,
        "IS_REGULARIZED": true,
        "LEARNING_RATE": 0.002,
        "MIN_MU_THRESHOLD": 0.2,
        "BINARIZE": true
      },
      "combinations": [
        "50,1000,3,3,0.1,0.0,10,500,16,0.5",
        "50,1000,3,5,0.1,0.0,10,500,16,0.5",
        "100,1000,3,3,0.1,0.0,10,500,16,0.5",
        "100,1000,3,5,0.1,0.0,10,500,16,0.5",
        "200,1000,3,3,0.1,0.0,10,500,16,0.5",
        "200,1000,3,5,0.1,0.0,10,500,16,0.5",
        "50,2000,3,3,0.1,0.0,10,500,16,0.5",
        "50,2000,3,5,0.1,0.0,10,500,16,0.5",
        "100,2000,3,3,0.1,0.0,10,500,16,0.5",
        "100,2000,3,5,0.1,0.0,10,500,16,0.5",
        "200,2000,3,3,0.1,0.0,10,500,16,0.5",
        "200,2000,3,5,0.1,0.0,10,500,16,0.5",
        "50,5000,3,3,0.1,0.0,10,500,16,0.5",
        "50,5000,3,5,0.1,0.0,10,500,16,0.5",
        "100,5000,3,3,0.1,0.0,10,500,16,0.5",
        "100,5000,3,5,0.1,0.0,10,500,16,0.5",
        "100,5000,3,10,0.1,0.0,10,500,16,0.5",
        "200,5000,3,3,0.1,0.0,10,500,16,0.5",
        "200,5000,3,5,0.1,0.0,10,500,16,0.5"
      ]
    },
    "tier3": {
      "name": "Sample-Rich Scenarios",
      "description": "Evaluates GEMSS performance in **traditional Machine Learning regimes** where the number of samples equals or exceeds the number of features (n \u2265 p). This serves as a control for comparative studies against conventional ML methods in well-powered statistical scenarios.",
      "algorithm_parameters": {
        "N_ITER": 4000,
        "PRIOR_TYPE": "sss",
        "STUDENT_DF": 1,
        "STUDENT_SCALE": 1.0,
        "VAR_SLAB": 100.0,
        "VAR_SPIKE": 0.1,
        "WEIGHT_SLAB": 0.9,
        "WEIGHT_SPIKE": 0.1,
        "IS_REGULARIZED": true,
        "LEARNING_RATE": 0.002,
        "MIN_MU_THRESHOLD": 0.2,
        "BINARIZE": true
      },
      "combinations": [
        "200,100,3,3,0.1,0.0,6,500,16,0.5",
        "200,100,3,5,0.1,0.0,6,500,16,0.5",
        "500,100,3,3,0.1,0.0,6,500,16,0.5",
        "500,100,3,5,0.1,0.0,6,500,16,0.5",
        "1000,100,3,3,0.1,0.0,6,500,16,0.5",
        "1000,100,3,5,0.1,0.0,6,500,16,0.5",
        "500,200,3,3,0.1,0.0,6,500,16,0.5",
        "500,200,3,5,0.1,0.0,6,500,16,0.5",
        "1000,200,3,3,0.1,0.0,6,500,16,0.5",
        "1000,200,3,5,0.1,0.0,6,500,16,0.5",
        "1000,500,3,3,0.1,0.0,6,500,16,0.5",
        "1000,500,3,5,0.1,0.0,6,500,16,0.5",
        "1000,500,3,10,0.1,0.0,6,500,16,0.5",
        "200,200,3,3,0.1,0.0,6,500,16,0.5",
        "200,200,3,5,0.1,0.0,6,500,16,0.5",
        "500,500,3,3,0.1,0.0,6,500,16,0.5",
        "500,500,3,5,0.1,0.0,6,500,16,0.5",
        "2000,500,3,3,0.1,0.0,6,500,16,0.5",
        "2000,500,3,5,0.1,0.0,6,500,16,0.5",
        "5000,500,3,5,0.1,0.0,6,500,16,0.5"
      ]
    },
    "tier4": {
      "name": "Robustness Under Adversity",
      "description": "Tests the algorithm's **stability under data quality challenges**, including high Gaussian noise (NOISE_STD) and random missing feature values (NAN_RATIO). Larger sample sizes (n \u2265 100) are used to maintain sufficient signal and demonstrate robustness despite data loss. BATCH_SIZE is dynamically scaled up for non-zero NAN_RATIO configurations.",
      "algorithm_parameters": {
        "N_ITER": 4000,
        "PRIOR_TYPE": "sss",
        "STUDENT_DF": 1,
        "STUDENT_SCALE": 1.0,
        "VAR_SLAB": 100.0,
        "VAR_SPIKE": 0.1,
        "WEIGHT_SLAB": 0.9,
        "WEIGHT_SPIKE": 0.1,
        "IS_REGULARIZED": true,
        "LEARNING_RATE": 0.002,
        "MIN_MU_THRESHOLD": 0.2,
        "BINARIZE": true
      },
      "combinations": [
        "100,100,3,3,0.1,0.0,6,500,16,0.5",
        "100,100,3,3,1.0,0.0,6,500,16,0.5",
        "100,100,3,3,2.0,0.0,6,500,16,0.5",
        "100,100,3,3,0.1,0.25,6,500,32,0.5",
        "100,100,3,3,0.1,0.5,6,500,48,0.5",
        "100,100,3,3,0.5,0.25,6,500,32,0.5",
        "100,100,3,3,0.5,0.5,6,500,48,0.5",
        "100,100,3,3,1.0,0.25,6,500,32,0.5",
        "100,100,3,3,1.0,0.5,6,500,48,0.5",
        "200,500,3,5,0.1,0.0,6,500,16,0.5",
        "200,500,3,5,1.0,0.0,6,500,16,0.5",
        "200,500,3,5,2.0,0.0,6,500,16,0.5",
        "200,500,3,5,0.1,0.25,6,500,32,0.5",
        "200,500,3,5,0.1,0.5,6,500,48,0.5",
        "200,500,3,5,0.1,0.75,6,500,96,0.5",
        "200,500,3,5,0.5,0.25,6,500,32,0.5",
        "200,500,3,5,0.5,0.5,6,500,48,0.5",
        "200,500,3,5,1.0,0.25,6,500,32,0.5",
        "200,500,3,5,1.0,0.5,6,500,48,0.5",
        "200,500,3,5,1.0,0.75,6,500,96,0.5",
        "1000,500,3,5,0.1,0.0,6,500,16,0.5",
        "1000,500,3,5,1.0,0.0,6,500,16,0.5",
        "1000,500,3,5,2.0,0.0,6,500,16,0.5",
        "1000,500,3,5,0.1,0.25,6,500,32,0.5",
        "1000,500,3,5,0.1,0.5,6,500,48,0.5",
        "1000,500,3,5,0.1,0.75,6,500,96,0.5",
        "1000,500,3,5,0.5,0.25,6,500,32,0.5",
        "1000,500,3,5,0.5,0.5,6,500,48,0.5",
        "1000,500,3,5,1.0,0.25,6,500,32,0.5",
        "1000,500,3,5,1.0,0.5,6,500,48,0.5",
        "1000,500,3,5,1.0,0.75,6,500,96,0.5"
      ]
    },
    "tier5": {
      "name": "Effect of Jaccard Penalty",
      "description": "Tests the influence of the regularization parameter **LAMBDA_JACCARD (\u03bb)** on the **diversity (overlap)** of the multiple sparse solutions discovered. Varies \u03bb from 0 (no diversity penalty) to 5000 (extreme penalty) across varied sample-feature regimes (n \u2248 p to n \u226a p).",
      "algorithm_parameters": {
        "N_ITER": 4000,
        "PRIOR_TYPE": "sss",
        "STUDENT_DF": 1,
        "STUDENT_SCALE": 1.0,
        "VAR_SLAB": 100.0,
        "VAR_SPIKE": 0.1,
        "WEIGHT_SLAB": 0.9,
        "WEIGHT_SPIKE": 0.1,
        "IS_REGULARIZED": true,
        "LEARNING_RATE": 0.002,
        "MIN_MU_THRESHOLD": 0.2,
        "BINARIZE": true
      },
      "combinations": [
        "100,100,3,3,0.1,0.0,6,0,16,0.5",
        "100,100,3,3,0.1,0.0,6,500,16,0.5",
        "100,100,3,3,0.1,0.0,6,1000,16,0.5",
        "100,100,3,3,0.1,0.0,6,5000,16,0.5",
        "100,100,3,5,0.1,0.0,6,0,16,0.5",
        "100,100,3,5,0.1,0.0,6,500,16,0.5",
        "100,100,3,5,0.1,0.0,6,1000,16,0.5",
        "100,100,3,5,0.1,0.0,6,5000,16,0.5",
        "200,500,3,3,0.1,0.0,6,0,16,0.5",
        "200,500,3,3,0.1,0.0,6,500,16,0.5",
        "200,500,3,3,0.1,0.0,6,1000,16,0.5",
        "200,500,3,3,0.1,0.0,6,5000,16,0.5",
        "200,500,3,5,0.1,0.0,6,0,16,0.5",
        "200,500,3,5,0.1,0.0,6,500,16,0.5",
        "200,500,3,5,0.1,0.0,6,1000,16,0.5",
        "200,500,3,5,0.1,0.0,6,5000,16,0.5",
        "200,500,3,10,0.1,0.0,6,0,16,0.5",
        "200,500,3,10,0.1,0.0,6,500,16,0.5",
        "200,500,3,10,0.1,0.0,6,1000,16,0.5",
        "200,500,3,10,0.1,0.0,6,5000,16,0.5",
        "500,300,3,3,0.1,0.0,6,0,16,0.5",
        "500,300,3,3,0.1,0.0,6,500,16,0.5",
        "500,300,3,3,0.1,0.0,6,1000,16,0.5",
        "500,300,3,3,0.1,0.0,6,5000,16,0.5",
        "500,300,3,5,0.1,0.0,6,0,16,0.5",
        "500,300,3,5,0.1,0.0,6,500,16,0.5",
        "500,300,3,5,0.1,0.0,6,1000,16,0.5",
        "500,300,3,5,0.1,0.0,6,5000,16,0.5",
        "500,300,3,10,0.1,0.0,6,0,16,0.5",
        "500,300,3,10,0.1,0.0,6,500,16,0.5",
        "500,300,3,10,0.1,0.0,6,1000,16,0.5",
        "500,300,3,10,0.1,0.0,6,5000,16,0.5"
      ]
    },
    "tier6": {
      "name": "Regression Validation (Robustness Grid)",
      "description": "Verifies the core functionality of GEMSS for **continuous response variables (regression)**. Uses a robustness grid (low/high noise, low/high missing data) to ensure stability in the regression context, mirroring configurations from Tier 1.",
      "algorithm_parameters": {
        "N_ITER": 4000,
        "PRIOR_TYPE": "sss",
        "STUDENT_DF": 1,
        "STUDENT_SCALE": 1.0,
        "VAR_SLAB": 100.0,
        "VAR_SPIKE": 0.1,
        "WEIGHT_SLAB": 0.9,
        "WEIGHT_SPIKE": 0.1,
        "IS_REGULARIZED": true,
        "LEARNING_RATE": 0.002,
        "MIN_MU_THRESHOLD": 0.2,
        "BINARIZE": false
      },
      "combinations": [
        "50,100,3,5,0.1,0.0,10,500,16,0.5",
        "100,200,3,5,0.1,0.0,10,500,16,0.5",
        "200,500,3,5,0.1,0.0,10,500,16,0.5",
        "500,500,3,5,0.1,0.0,10,500,16,0.5",
        "50,100,3,5,1.0,0.0,10,500,16,0.5",
        "100,200,3,5,1.0,0.0,10,500,16,0.5",
        "200,500,3,5,1.0,0.0,10,500,16,0.5",
        "500,500,3,5,1.0,0.0,10,500,16,0.5",
        "50,100,3,5,0.1,0.5,10,500,48,0.5",
        "100,200,3,5,0.1,0.5,10,500,48,0.5",
        "200,500,3,5,0.1,0.5,10,500,48,0.5",
        "500,500,3,5,0.1,0.5,10,500,48,0.5",
        "50,100,3,5,1.0,0.5,10,500,48,0.5",
        "100,200,3,5,1.0,0.5,10,500,48,0.5",
        "200,500,3,5,1.0,0.5,10,500,48,0.5",
        "500,500,3,5,1.0,0.5,10,500,48,0.5"
      ]
    },
    "tier7": {
      "name": "Unreliable and Unbalanced Responses",
      "description": "Tests stability and feature selection when dealing with **response variable issues**, specifically highly **unbalanced class ratios** (BINARY_RESPONSE_RATIO \u226a 0.5) and interactions with light data quality issues (NAN_RATIO).",
      "algorithm_parameters": {
        "N_ITER": 4000,
        "PRIOR_TYPE": "sss",
        "STUDENT_DF": 1,
        "STUDENT_SCALE": 1.0,
        "VAR_SLAB": 100.0,
        "VAR_SPIKE": 0.1,
        "WEIGHT_SLAB": 0.9,
        "WEIGHT_SPIKE": 0.1,
        "IS_REGULARIZED": true,
        "LEARNING_RATE": 0.002,
        "MIN_MU_THRESHOLD": 0.2,
        "BINARIZE": true
      },
      "combinations": [
        "100,100,3,5,0.1,0.0,10,500,16,0.2",
        "100,100,3,5,0.1,0.0,10,500,16,0.1",
        "200,500,3,5,0.1,0.0,10,500,16,0.2",
        "200,500,3,5,0.1,0.0,10,500,16,0.1",
        "100,100,3,5,0.5,0.0,10,500,16,0.2",
        "100,100,3,5,0.5,0.0,10,500,16,0.1",
        "200,500,3,5,0.5,0.0,10,500,16,0.2",
        "200,500,3,5,0.5,0.0,10,500,16,0.1",
        "100,100,3,5,0.1,0.1,10,500,26,0.5",
        "100,100,3,5,0.1,0.2,10,500,30,0.5",
        "200,500,3,5,0.1,0.1,10,500,26,0.5",
        "200,500,3,5,0.1,0.2,10,500,30,0.5"
      ]
    }
  },
  "parameter_format": "N_SAMPLES,N_FEATURES,N_GENERATING_SOLUTIONS,SPARSITY,NOISE_STD,NAN_RATIO,N_CANDIDATE_SOLUTIONS,LAMBDA_JACCARD,BATCH_SIZE,BINARY_RESPONSE_RATIO",
  "fixed_parameters": {
    "DATASET_SEED": 42,
    "SAMPLE_MORE_PRIORS_COEFF": 1.0,
    "USE_MEDIAN_FOR_OUTLIER_DETECTION": false,
    "OUTLIER_DEVIATION_THRESHOLDS": [2.0, 2.5, 3.0]
  },
  "notes": {
    "version": "7-tier optimized design (Batch Size Included)",
    "total_experiments": 154,
    "design_changes": [
      "All combinations now explicitly include BATCH_SIZE and BINARY_RESPONSE_RATIO (10 parameters total).",
      "BATCH_SIZE is calculated as: 16 for NAN_RATIO=0.0; int(16 * 1.5 / (1 - NAN_RATIO)) for NAN_RATIO > 0.",
      "Sparsity k=10 usage is minimized (16 times total across all tiers).",
      "Robustness Tier 4 sample sizes (N) are increased to safely handle high NAN_RATIO=0.75 scenarios."
    ],
    "sparsity_focus": "Ultra-sparse signals: 3 and 5 features are dominant."
  }
}