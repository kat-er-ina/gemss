{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14286eb9",
   "metadata": {},
   "source": [
    "# GEMSS on your custom dataset\n",
    "\n",
    "This notebook demonstrates how to apply GEMSS to your own dataset with unknown ground truth. It largely mirrors the workflow of demo.ipynb, where applicable.\n",
    "\n",
    "Use this notebook to try out GEMSS on your custom data. Follow the instructions below to set up the algorithm's parameters.\n",
    "\n",
    "When an experiment is run, the hyperparameters, search history and results are saved into a predefined directory (variable \"experiment_id\") and can be loaded again by this notebook (variable \"run_or_load\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c868d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "import numpy as np\n",
    "from plotly import io as pio\n",
    "\n",
    "import gemss.config as C\n",
    "from gemss.config import display_current_config\n",
    "from gemss.utils.utils import (\n",
    "    show_solution_summary,\n",
    "    display_feature_lists,\n",
    "    save_feature_lists_txt,\n",
    "    save_feature_lists_json,\n",
    "    save_selector_history_json,\n",
    "    save_constants_json,\n",
    "    load_selector_history_json,\n",
    "    load_constants_json,\n",
    ")\n",
    "from gemss.feature_selection.inference import BayesianFeatureSelector\n",
    "from gemss.utils.visualizations import (\n",
    "    show_label_histogram,\n",
    "    show_final_alphas,\n",
    "    show_features_in_components,\n",
    "    show_algorithm_progress,\n",
    ")\n",
    "from gemss.postprocessing.result_postprocessing import (\n",
    "    get_features_from_solutions,\n",
    "    get_unique_features,\n",
    "    recover_solutions,\n",
    "    show_solution_summary,\n",
    "    show_unique_features_from_full_solutions,\n",
    ")\n",
    "from gemss.postprocessing.simple_regressions import (\n",
    "    solve_any_regression,\n",
    "    show_regression_metrics,\n",
    ")\n",
    "from gemss.diagnostics.recommendations import display_parameter_adjustment_summary\n",
    "from gemss.utils.utils import show_solution_summary\n",
    "\n",
    "from gemss.data_handling.data_processing import (\n",
    "    load_data,\n",
    "    get_df_from_X,\n",
    "    preprocess_features,\n",
    ")\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"  # Ensures plotly plots show in notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8879eb36",
   "metadata": {},
   "source": [
    "# Your setup\n",
    "\n",
    "In this section, define the specifics for your data and parameters for the feature selection algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df62a6",
   "metadata": {},
   "source": [
    "## Dataset parameters\n",
    "\n",
    "Set up the name of your dataset and the names of the index and label columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef0de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the CSV file should be in the ../data/ directory\n",
    "# the index and label column names must be included in the dataset\n",
    "csv_dataset_name = \"my_custom_dataset.csv\"\n",
    "index_column_name = \"sample ID\"\n",
    "label_column_name = \"label\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109fd3f",
   "metadata": {},
   "source": [
    "## Data processing parameters\n",
    "\n",
    "- **Handling missing values:** By default, only rows with missing labels/response values are dropped and other missing values are handled natively. Other options are available.\n",
    "- **Handling non-numerical features:** Currently, only numerical features are supported.\n",
    "- **Feature scaling:** If None, major changes in parameter settings might be necessary. It is highly advised to normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8233ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA handling options\n",
    "# Options are:\n",
    "# - \"response\": drop rows with NA in the response column only (default).\n",
    "# - \"all\": drop rows with NA in any column.\n",
    "# - \"none\": do not drop any rows.\n",
    "dropna_columns = \"response\"\n",
    "\n",
    "# the threshold for allowed percentage of missing values in each feature\n",
    "# features with a higher percentage of missing values will be dropped\n",
    "# either None or a fraction or a value between 0 and 100\n",
    "allowed_missing_percentage = 20\n",
    "\n",
    "# If True, keep only numerical features\n",
    "drop_non_numeric_features = True\n",
    "\n",
    "# Optionally, apply scaling to features\n",
    "# Used in both feature selection algorithm\n",
    "# and the simple regression models on the results\n",
    "apply_scaling = \"standard\"  # options: \"standard\", \"minmax\", None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5926f4d4",
   "metadata": {},
   "source": [
    "### Choose whether to run the optimization or load saved results\n",
    "\n",
    "Either run the feature selector (the history and configuration are saved) or load a specified experiment.\n",
    "\n",
    "This part enables a user to run, save and view multiple experiments on their custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c056711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'run' => run the feature selector + save the history and configuration\n",
    "# 'load' => load the optimization configuration + history from file\n",
    "run_or_load = \"run\"\n",
    "\n",
    "# select experiment number\n",
    "experiment_id = 1\n",
    "\n",
    "# recommended place to to save your experiments (retrieved by reruns or evaluation of results)\n",
    "experiment_dir = f\"./results/experiment_{experiment_id}\"\n",
    "os.makedirs(experiment_dir, exist_ok=True)\n",
    "history_filename = f\"{experiment_dir}/search_history_results.json\"\n",
    "constants_filename = f\"{experiment_dir}/search_setup.json\"\n",
    "features_filename_json = f\"{experiment_dir}/all_candidate_solutions.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86cb148",
   "metadata": {},
   "source": [
    "### Govern verbosity and outputting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a3ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plots of algorithm progress over iterations\n",
    "show_search_history = True\n",
    "\n",
    "# Choose overall verbosity for various outputs\n",
    "verbose = True\n",
    "\n",
    "# Whether to run regressions for every set of solutions and show the summary metrics\n",
    "run_regressions_for_solutions = True\n",
    "\n",
    "# one chosen type of candidate solutions to be analyzed in greater detail (for each component)\n",
    "# can be one of these:\n",
    "# None   ... no detailed analysis\n",
    "# 'Full features' ... solutions consisting of all features above the provided minimal |mu| threshold\n",
    "# 'Top features' ... the top few features according to |mu| values. Number determined by 'desired sparsity'\n",
    "# 'Outlier features (STD_{deviation})'\n",
    "#       ... {deviation} must be a number in the OUTLIER_DEVIATION_THRESHOLDS list set up below\n",
    "#       ... e.g. 'Outlier features (STD_3.0)' for solutions given by outlier detection with 3.0*STD threshold\n",
    "chosen_solution_type = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aae0a7",
   "metadata": {},
   "source": [
    "## Load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this cell\n",
    "\n",
    "if verbose:\n",
    "    display(Markdown(\"#### Loading your data\"))\n",
    "\n",
    "df, response = load_data(\n",
    "    csv_dataset_name,\n",
    "    index_column_name,\n",
    "    label_column_name,\n",
    ")\n",
    "\n",
    "X, y, feature_to_name = preprocess_features(\n",
    "    df,\n",
    "    response,\n",
    "    dropna=dropna_columns,\n",
    "    allowed_missing_percentage=allowed_missing_percentage,\n",
    "    drop_non_numeric_features=drop_non_numeric_features,\n",
    "    apply_scaling=apply_scaling,\n",
    "    verbose=verbose,\n",
    ")\n",
    "df = get_df_from_X(X, feature_to_name)\n",
    "\n",
    "overall_nan_ratio = np.isnan(X).sum() / (X.shape[0] * X.shape[1])\n",
    "if verbose:\n",
    "    display(Markdown(\"#### Your data:\"))\n",
    "    display(Markdown(f\"- **Number of labels:** {len(np.unique(y))}\"))\n",
    "    show_label_histogram(y)\n",
    "    display(Markdown(f\"- **Number of samples:** {X.shape[0]}\"))\n",
    "    display(Markdown(f\"- **Number of features:** {X.shape[1]}\"))\n",
    "    display(Markdown(f\"{list(feature_to_name.values())}\"))\n",
    "    display(\n",
    "        Markdown(\n",
    "            f\"- **Total percentage of missing values:** {overall_nan_ratio * 100:.1f} %\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d0b861",
   "metadata": {},
   "source": [
    "## Set parameters for the feature selection algorithm\n",
    "\n",
    "- First, default contant values are loaded by the config module.\n",
    "- Then override the settings of select parameters as needed.\n",
    "\n",
    "- The algorithm usually takes about 1+ minute per 1000 training iterations on CPU for the default 'sss' prior. The 'student' prior is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aa6f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_or_load == \"load\":\n",
    "    # loading the setup from the specified json\n",
    "    constants, msg = load_constants_json(constants_filename)\n",
    "    display(Markdown(msg))\n",
    "\n",
    "else:\n",
    "    # First load the default constants defined by the config module (including nonsensical values for your data)\n",
    "    # And change the selected values below\n",
    "    # This ensures that there is nothing missing\n",
    "    constants = C.as_dict()\n",
    "    constants[\"N_SAMPLES\"] = X.shape[0]\n",
    "    constants[\"N_FEATURES\"] = X.shape[1]\n",
    "\n",
    "    # ----------------------- SETUP OF THE FEATURE SELECTION PROCEDURE ------------------------------------------#\n",
    "\n",
    "    # SPARSIITY = expected number of features per solution\n",
    "    # only used to set up parameters PRIOR_SPARSITY and DESIRED_SPARSITY\n",
    "    # to an identical value (can be overridden below if needed)\n",
    "    constants[\"SPARSITY\"] = 4\n",
    "\n",
    "    # =======================================================\n",
    "    #     Algorithm settings\n",
    "    # =======================================================\n",
    "\n",
    "    ### PRIOR_TYPE\n",
    "    ### ...do not change unless you know what you are doing or nothing else works\n",
    "    ### ...SSS = structured spike and slab priors\n",
    "    ### ...method of 2nd recourse: 'student'\n",
    "    constants[\"PRIOR_TYPE\"] = \"sss\"\n",
    "\n",
    "    ### PRIOR_SPARSITY = number of supporting dimensions for the 'sss' prior\n",
    "    ### used only with the 'sss' prior\n",
    "    constants[\"PRIOR_SPARSITY\"] = constants[\n",
    "        \"SPARSITY\"\n",
    "    ]  # leave this setting unless there is a special reason to change it\n",
    "\n",
    "    ### VAR_SPIKE and VAR_SLAB are only used with 'ss' and 'sss' prior\n",
    "    ### ...VAR_SPIKE: parameter with the most influence on solution quality\n",
    "    ### ...smaller VAR_SPIKE => more sparsity, i.e. fewer nonzero solutions\n",
    "    ### ...increase VAR_SPIKE when all features converge to 0, typically in a uniform manner\n",
    "    ### ...decrease VAR_SPIKE when there are too many nonzero features at the end of the run\n",
    "    constants[\"VAR_SPIKE\"] = 0.1\n",
    "    ## Prior hyperparameters\n",
    "    constants[\"VAR_SLAB\"] = 100.0\n",
    "\n",
    "    ### WEIGHT_SLAB and WEIGHT_SPIKE are only used with 'ss' prior\n",
    "    constants[\"WEIGHT_SLAB\"] = 0.9\n",
    "    constants[\"WEIGHT_SPIKE\"] = 0.1\n",
    "\n",
    "    ### STUDENT_DF and STUDENT_SCALE are only used with the Student prior\n",
    "    # Degrees of freedom (ν): controls tail heaviness and how “robust” the distribution is:\n",
    "    #   low ν → very heavy tails, high ν → approaches Normal\n",
    "    #   - Start moderately (e.g., 3–10). Very low (1–2) can slow convergence and produce very erratic large coefficients\n",
    "    #   - Increase ν if solutions are too noisy or unstable\n",
    "    #   - Decrease ν if you want to allow rare large effects\n",
    "    # Scale (s): stretches or contracts the distribution\n",
    "    #   acts like standard deviation when variance exists (ν > 2)\n",
    "    #   - Decrease if too many features end up nonzero (tighten prior around zero).\n",
    "    #   - Increase if all coefficients collapse near zero (prior overly restrictive).\n",
    "    #   - Adjust s before ν if the issue is general spread rather than tail behavior.\n",
    "    constants[\"STUDENT_DF\"] = 5\n",
    "    constants[\"STUDENT_SCALE\"] = 1.0\n",
    "\n",
    "    ## SGD optimization settings\n",
    "    ### N_ITER = number of training iterations\n",
    "    ### increase/decrease depending on both the ELBO and the convergence behavior of μs\n",
    "    constants[\"N_ITER\"] = 4000\n",
    "\n",
    "    ## Regularization settings to make solutions more distinct\n",
    "    ### IS_REGULARIZED = whether to use penalization based on average Jaccard similarity among solutions\n",
    "    ### ...preferrably False or True with small LAMBDA_JACCARD at this moment\n",
    "    constants[\"IS_REGULARIZED\"] = True\n",
    "    ### LAMBDA_JACCARD: larger values => more differentiated feature sets\n",
    "    ### ...it is safer to use smaller values and increase only if needed\n",
    "    ### ...if the ELBO converges to a too large value (in absolute terms), your lambda is probably too large\n",
    "    constants[\"LAMBDA_JACCARD\"] = 1000.0\n",
    "\n",
    "    ## SGD optimization settings\n",
    "    ## ...smaller learning rates require more iterations to converge but may yield better optimization processes\n",
    "    ## ...batch size should be increased:\n",
    "    #       - for larger datasets (to speed up convergence)\n",
    "    #       - for datasets with a large percentage of missing values (to have enough complete data in each batch)\n",
    "    #       - for datasets with small minory class sizes (to have enough samples of each class in every batch)\n",
    "    constants[\"LEARNING_RATE\"] = 0.002\n",
    "    constants[\"BATCH_SIZE\"] = int(np.min([16, constants[\"N_SAMPLES\"]]))\n",
    "\n",
    "    # ===============================================================\n",
    "    #     Properties of the desired solutions and postprocessing\n",
    "    # ===============================================================\n",
    "\n",
    "    ### N_CANDIDATE_SOLUTIONS = number of candidate solutions\n",
    "    ###                       = no. components of Gaussian mixture that approximate the posterior\n",
    "    ### ...recommended to set as 2-3 times the expected number of solutions\n",
    "    constants[\"N_CANDIDATE_SOLUTIONS\"] = 8\n",
    "    # Expected # of features per solution\n",
    "    constants[\"DESIRED_SPARSITY\"] = constants[\n",
    "        \"SPARSITY\"\n",
    "    ]  # if you want to change it, change the SPARSITY value above\n",
    "\n",
    "    ### MIN_MU_THRESHOLD = minimum |μ| to consider a feature nonzero\n",
    "    ### ...adjust based on the scale of your features and the convergence behavior\n",
    "    constants[\"MIN_MU_THRESHOLD\"] = 0.25\n",
    "\n",
    "    # Settings for outlier detection\n",
    "    # ...affect only outlier-based solution sets\n",
    "    constants[\"USE_MEDIAN_FOR_OUTLIER_DETECTION\"] = False  # recommended\n",
    "    constants[\"OUTLIER_DEVIATION_THRESHOLDS\"] = [2.0, 2.5, 3.0]\n",
    "\n",
    "    # -------------------- NO NEED TO TOUCH CODE BELOW THIS CELL ---------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7165837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run or loaded, show the configuration\n",
    "display_current_config(\n",
    "    constants=constants,\n",
    "    constant_type=\"algorithm_and_postprocessing\",\n",
    ")\n",
    "display_parameter_adjustment_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c01117",
   "metadata": {},
   "source": [
    "# Run the feature selector on your data\n",
    "\n",
    "There is no need to touch any code below this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34906baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_or_load == \"run\":\n",
    "    # define the feature selector\n",
    "    selector = BayesianFeatureSelector(\n",
    "        n_features=constants[\"N_FEATURES\"],\n",
    "        n_components=constants[\"N_CANDIDATE_SOLUTIONS\"],\n",
    "        X=X,\n",
    "        y=y,\n",
    "        prior=constants[\"PRIOR_TYPE\"],\n",
    "        sss_sparsity=constants[\"PRIOR_SPARSITY\"],\n",
    "        var_slab=constants[\"VAR_SLAB\"],\n",
    "        var_spike=constants[\"VAR_SPIKE\"],\n",
    "        weight_slab=constants[\"WEIGHT_SLAB\"],\n",
    "        weight_spike=constants[\"WEIGHT_SPIKE\"],\n",
    "        student_df=constants[\"STUDENT_DF\"],\n",
    "        student_scale=constants[\"STUDENT_SCALE\"],\n",
    "        lr=constants[\"LEARNING_RATE\"],\n",
    "        batch_size=constants[\"BATCH_SIZE\"],\n",
    "        n_iter=constants[\"N_ITER\"],\n",
    "    )\n",
    "    # run the selector\n",
    "    history = selector.optimize(\n",
    "        regularize=constants[\"IS_REGULARIZED\"],\n",
    "        lambda_jaccard=constants[\"LAMBDA_JACCARD\"],\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    msg = save_selector_history_json(history, history_filename)\n",
    "    display(Markdown(msg))\n",
    "    msg = save_constants_json(constants, constants_filename)\n",
    "    display(Markdown(msg))\n",
    "\n",
    "else:\n",
    "    history, msg = load_selector_history_json(history_filename)\n",
    "    display(Markdown(msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9c908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_search_history:\n",
    "    show_algorithm_progress(\n",
    "        history,\n",
    "        original_feature_names_mapping=feature_to_name,\n",
    "    )\n",
    "\n",
    "    show_final_alphas(\n",
    "        history,\n",
    "        show_bar_plot=False,\n",
    "        show_pie_chart=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa4da6a",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Analyze the feature selection process to extract solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f8edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_solutions, top_solutions, outlier_solutions, final_parameters = recover_solutions(\n",
    "    search_history=history,\n",
    "    desired_sparsity=constants[\"DESIRED_SPARSITY\"],\n",
    "    min_mu_threshold=constants[\"MIN_MU_THRESHOLD\"],\n",
    "    original_feature_names_mapping=feature_to_name,\n",
    "    use_median_for_outlier_detection=constants[\"USE_MEDIAN_FOR_OUTLIER_DETECTION\"],\n",
    "    outlier_deviation_thresholds=constants[\"OUTLIER_DEVIATION_THRESHOLDS\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21569d44",
   "metadata": {},
   "source": [
    "## Solutions by outlier detection\n",
    "\n",
    "The outlier analysis helps identify features with unusually high importance values (mu, either positive or negative) in each component.\n",
    "\n",
    "Ideally, the detected outliers are identical to the top solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff7858",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_features = {}\n",
    "for deviation, df_outlier in outlier_solutions.items():\n",
    "    show_solution_summary(\n",
    "        solution_data=df_outlier,\n",
    "        title=f\"Outlier solutions for {deviation}\",\n",
    "        value_column=\"Feature\",\n",
    "    )\n",
    "\n",
    "    show_unique_features_from_full_solutions(df_outlier)\n",
    "\n",
    "    outlier_features[f\"{deviation}\"] = get_features_from_solutions(df_outlier)\n",
    "    show_features_in_components(\n",
    "        solutions=outlier_features[f\"{deviation}\"],\n",
    "        features_to_show=get_unique_features(outlier_features[f\"{deviation}\"]),\n",
    "    )\n",
    "\n",
    "    if run_regressions_for_solutions:\n",
    "        penalty = \"l2\"\n",
    "        metrics = solve_any_regression(\n",
    "            solutions=outlier_features[f\"{deviation}\"],\n",
    "            df=df,\n",
    "            response=response,\n",
    "            apply_scaling=apply_scaling,\n",
    "            penalty=penalty,\n",
    "            verbose=False,\n",
    "        )\n",
    "        show_regression_metrics(\n",
    "            metrics_df=metrics,\n",
    "            title=f\"Outlier features ({deviation}) - regression results on training data (penalty={penalty})\",\n",
    "            use_markdown=True,\n",
    "        )\n",
    "\n",
    "    display(Markdown(\"------------------------------------------------------------\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3067318f",
   "metadata": {},
   "source": [
    "## Full solutions\n",
    "\n",
    "The 'full' solutions are the actual solutions (full sets of features) found by the algorithm. Their sparsity may not be as strong as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_solution_summary(\n",
    "    solution_data=full_solutions,\n",
    "    title=\"Full solutions found by the feature selector, ordered by importance\",\n",
    "    value_column=\"Feature\",\n",
    ")\n",
    "\n",
    "show_unique_features_from_full_solutions(full_solutions)\n",
    "\n",
    "full_features = get_features_from_solutions(full_solutions)\n",
    "show_features_in_components(\n",
    "    solutions=full_features,\n",
    "    features_to_show=get_unique_features(full_features),\n",
    ")\n",
    "\n",
    "if run_regressions_for_solutions:\n",
    "    penalty = \"l2\"\n",
    "    metrics = solve_any_regression(\n",
    "        solutions=full_features,\n",
    "        df=df,\n",
    "        response=response,\n",
    "        apply_scaling=apply_scaling,\n",
    "        penalty=penalty,\n",
    "        verbose=False,\n",
    "    )\n",
    "    show_regression_metrics(\n",
    "        metrics_df=metrics,\n",
    "        title=f\"Full features - regression results on training data (penalty={penalty})\",\n",
    "        use_markdown=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e095bd",
   "metadata": {},
   "source": [
    "## Top solutions\n",
    "\n",
    "The top solutions are just the most important features from the long solutions. The number of features selected is defined by the desired sparsity parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1979681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"### Required sparsity: {constants[\"DESIRED_SPARSITY\"]}\"))\n",
    "\n",
    "show_solution_summary(\n",
    "    solution_data=top_solutions,\n",
    "    title=f\"Top solutions with required sparsity {constants[\"DESIRED_SPARSITY\"]}\",\n",
    "    value_column=\"Feature\",\n",
    ")\n",
    "\n",
    "show_unique_features_from_full_solutions(top_solutions)\n",
    "\n",
    "top_features = get_features_from_solutions(top_solutions)\n",
    "show_features_in_components(\n",
    "    solutions=top_features,\n",
    "    features_to_show=get_unique_features(top_features),\n",
    ")\n",
    "\n",
    "if run_regressions_for_solutions:\n",
    "    penalty = \"l2\"\n",
    "    metrics = solve_any_regression(\n",
    "        solutions=top_features,\n",
    "        df=df,\n",
    "        response=response,\n",
    "        apply_scaling=apply_scaling,\n",
    "        penalty=penalty,\n",
    "        verbose=False,\n",
    "    )\n",
    "    show_regression_metrics(\n",
    "        metrics_df=metrics,\n",
    "        title=f\"Top features - regression results on training data (penalty={penalty})\",\n",
    "        use_markdown=True,\n",
    "    )\n",
    "\n",
    "display(Markdown(\"------------------------------------------------------------\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850f35a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unify all feature lists for saving and displaying into a single dictionary\n",
    "all_feature_dicts = [top_features, full_features] + list(outlier_features.values())\n",
    "feature_dict_titles = [\n",
    "    \"Top features\",\n",
    "    \"Full features\",\n",
    "] + [f\"Outlier features ({deviation})\" for deviation in outlier_features.keys()]\n",
    "\n",
    "# Create a dictionary mapping feature list titles to their corresponding feature lists\n",
    "# Structure: {title: {component_index: [features]}}\n",
    "all_features_lists = {\n",
    "    feature_dict_titles[i]: all_feature_dicts[i] for i in range(len(all_feature_dicts))\n",
    "}\n",
    "\n",
    "display(Markdown(f\"## All available solutions\"))\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"**The following solution types are available:** {list(all_features_lists.keys())}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "if verbose:\n",
    "    for title, feature_dict in all_features_lists.items():\n",
    "        display_feature_lists(\n",
    "            feature_dict,\n",
    "            title=title,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd69c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save all candidate solutions\n",
    "\n",
    "if run_or_load == \"run\":\n",
    "    msg = save_feature_lists_json(\n",
    "        all_features_lists,\n",
    "        features_filename_json,\n",
    "    )\n",
    "    display(Markdown(msg))\n",
    "\n",
    "    # saving to txt for a well-readable file\n",
    "    msg = save_feature_lists_txt(\n",
    "        all_features_lists,\n",
    "        features_filename_json.replace(\".json\", \".txt\"),\n",
    "    )\n",
    "    display(Markdown(msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc3392",
   "metadata": {},
   "source": [
    "# More detailed analysis of the chosen solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23078391",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"**Available solution types:** {list(all_features_lists.keys())}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633c271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_solution_type = \"Outlier features (STD_2.0)\"\n",
    "if chosen_solution_type is not None:\n",
    "    display(\n",
    "        Markdown(\n",
    "            f\"## Regression for the chosen solution type **{chosen_solution_type}**\"\n",
    "        )\n",
    "    )\n",
    "    metrics = solve_any_regression(\n",
    "        solutions=all_features_lists[chosen_solution_type],\n",
    "        df=df,\n",
    "        response=response,\n",
    "        apply_scaling=apply_scaling,\n",
    "        penalty=penalty,\n",
    "        verbose=True,  # will show all details including the metrics summary\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83803db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
