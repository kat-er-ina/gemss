{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14286eb9",
   "metadata": {},
   "source": [
    "# GEMSS on your custom dataset\n",
    "\n",
    "This notebook demonstrates how to apply GEMSS to your own dataset with unknown ground truth. It largely mirrors the workflow of demo.ipynb, where applicable.\n",
    "\n",
    "Use this notebook to try out GEMSS on your custom data. Follow the instructions below to set up the algorithm's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58470f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -e .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c868d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotly import io as pio\n",
    "\n",
    "import gemss.config as C\n",
    "from gemss.feature_selection.inference import BayesianFeatureSelector\n",
    "from gemss.diagnostics.visualizations import (\n",
    "    show_label_histogram,\n",
    "    show_final_alphas,\n",
    "    show_features_in_components,\n",
    ")\n",
    "from gemss.diagnostics.outliers import show_outlier_features_by_component\n",
    "from gemss.diagnostics.result_postprocessing import (\n",
    "    recover_solutions,\n",
    "    show_algorithm_progress,\n",
    "    show_unique_features,\n",
    "    show_unique_features_from_full_solutions,\n",
    "    get_features_from_long_solutions,\n",
    "    get_unique_features,\n",
    ")\n",
    "from gemss.diagnostics.simple_regressions import show_regression_results_for_solutions\n",
    "from gemss.diagnostics.performance_tests import run_performance_diagnostics\n",
    "from gemss.diagnostics.recommendations import display_recommendations\n",
    "from gemss.utils import show_solution_summary\n",
    "\n",
    "from gemss.data_handling.data_processing import (\n",
    "    load_data,\n",
    "    preprocess_non_numeric_features,\n",
    "    preprocess_features,\n",
    ")\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"  # Ensures plotly plots show in notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8879eb36",
   "metadata": {},
   "source": [
    "# Your setup\n",
    "\n",
    "In this section, define the specifics for your data and parameters for the feature selection algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fddf904",
   "metadata": {},
   "source": [
    "## Govern verbosity and outputting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a3ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plots of algorithm progress over iterations\n",
    "show_search_history = True\n",
    "\n",
    "# Detect outlier features (in each component) by statistical means\n",
    "# Ideally, outliers == solutions\n",
    "outlier_analysis = True\n",
    "\n",
    "# Choose overall verbosity for various outputs\n",
    "verbose = True\n",
    "\n",
    "# Whether to run regressions for the selected solutions and show results\n",
    "run_regressions_for_solutions = True\n",
    "\n",
    "# Whether to run performance diagnostics\n",
    "# The diagnostics is necessary for showing recommendations\n",
    "# but can be run quietly, if desired\n",
    "run_diagnostics = False\n",
    "verbose_diagnostics = True\n",
    "\n",
    "# Whether to show recommendations based on diagnostics\n",
    "# requires `run_diagnostics = True` too\n",
    "show_recommendations = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df62a6",
   "metadata": {},
   "source": [
    "## Set parameters to handle your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8233ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset parameters\n",
    "# the CSV file should be in the ../data/ directory\n",
    "# the index and label column names must be included in the dataset\n",
    "# csv_dataset_name = \"shelflife_data_all_preprocessed.csv\"\n",
    "csv_dataset_name = \"shelflife_data_all.csv\"\n",
    "index_column_name = \"sample ID\"\n",
    "label_column_name = \"label\"\n",
    "\n",
    "# NA handling options\n",
    "# Options are:\n",
    "# - \"response\": drop rows with NA in the response column only (default).\n",
    "# - \"all\": drop rows with NA in any column.\n",
    "# - \"none\": do not drop any rows.\n",
    "dropna_columns = \"response\"\n",
    "\n",
    "# If True, keep only numerical features\n",
    "drop_non_numeric_features = True\n",
    "\n",
    "# Apply standard scaling to features\n",
    "apply_standard_scaling = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aae0a7",
   "metadata": {},
   "source": [
    "### Load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    display(Markdown(\"#### Loading your data\"))\n",
    "\n",
    "df, response = load_data(\n",
    "    csv_dataset_name,\n",
    "    index_column_name,\n",
    "    label_column_name,\n",
    ")\n",
    "\n",
    "if drop_non_numeric_features:\n",
    "    df = preprocess_non_numeric_features(df, how=\"drop\", verbose=verbose)\n",
    "\n",
    "X, y, feature_to_name = preprocess_features(\n",
    "    df,\n",
    "    response,\n",
    "    dropna=dropna_columns,\n",
    "    apply_standard_scaling=apply_standard_scaling,\n",
    "    verbose=verbose,\n",
    ")\n",
    "\n",
    "if verbose:\n",
    "    display(Markdown(\"#### Your data:\"))\n",
    "    display(Markdown(f\"- Number of samples: **{X.shape[0]}**\"))\n",
    "    display(Markdown(f\"- Number of features: **{X.shape[1]}**\"))\n",
    "    display(Markdown(f\"{list(feature_to_name.values())}\"))\n",
    "    display(Markdown(f\"- Number of labels: **{len(np.unique(y))}**\"))\n",
    "    show_label_histogram(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d0b861",
   "metadata": {},
   "source": [
    "## Set parameters for the feature selection algorithm\n",
    "\n",
    "- First, default contant values are loaded by the config module.\n",
    "- Then override the settings of select parameters as needed.\n",
    "\n",
    "- The algorithm usually takes about 1+ minute per 1000 training iterations on CPU for the default 'sss' prior. The 'student' prior is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9590dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL unless you know what you're doing\n",
    "\n",
    "# First load the default constants defined by the config module (including nonsensical values for your data)\n",
    "constants = C.as_dict()\n",
    "\n",
    "constants[\"N_SAMPLES\"] = X.shape[0]\n",
    "constants[\"N_FEATURES\"] = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66e3ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET YOUR PARAMETERS HERE\n",
    "\n",
    "# Algorithm settings\n",
    "\n",
    "### PRIOR_TYPE\n",
    "### ...do not change unless you know what you are doing or nothing else works\n",
    "### ...SSS = structured spike and slab priors\n",
    "### ...method of 2nd recourse: 'student'\n",
    "constants[\"PRIOR_TYPE\"] = \"sss\"\n",
    "\n",
    "### PRIOR_SPARSITY = number of supporting dimensions for the 'sss' prior\n",
    "### used only with the 'sss' prior\n",
    "constants[\"PRIOR_SPARSITY\"] = constants[\"DESIRED_SPARSITY\"]\n",
    "\n",
    "### VAR_SPIKE and VAR_SLAB are only used with 'ss' and 'sss' prior\n",
    "### ...VAR_SPIKE: parameter with the most influence on solution quality\n",
    "### ...smaller VAR_SPIKE => more sparsity, i.e. fewer nonzero solutions\n",
    "### ...increase VAR_SPIKE when all features converge to 0, typically in a uniform manner\n",
    "### ...decrease VAR_SPIKE when there are too many nonzero features at the end of the run\n",
    "constants[\"VAR_SPIKE\"] = 0.1\n",
    "## Prior hyperparameters\n",
    "constants[\"VAR_SLAB\"] = 100.0\n",
    "\n",
    "### WEIGHT_SLAB and WEIGHT_SPIKE are only used with 'ss' prior\n",
    "constants[\"WEIGHT_SLAB\"] = 0.9\n",
    "constants[\"WEIGHT_SPIKE\"] = 0.1\n",
    "\n",
    "### STUDENT_DF and STUDENT_SCALE are only used with the Student prior\n",
    "constants[\"STUDENT_DF\"] = 1\n",
    "constants[\"STUDENT_SCALE\"] = 1.0\n",
    "\n",
    "## SGD optimization settings\n",
    "### N_ITER = number of training iterations (runtime approximately 1+ minutes/1000 iterations)\n",
    "### increase/decrease depending on both the ELBO and the convergence behavior of μs\n",
    "constants[\"N_ITER\"] = 1500\n",
    "\n",
    "## Regularization settings to make solutions more distinct\n",
    "### IS_REGULARIZED = whether to use penalization based on average Jaccard similarity among solutions\n",
    "### ...preferrably False or True with small LAMBDA_JACCARD at this moment\n",
    "constants[\"IS_REGULARIZED\"] = True\n",
    "### LAMBDA_JACCARD: larger values => more differentiated feature sets\n",
    "### ...it is safer to use smaller values and increase only if needed\n",
    "### ...if the ELBO converges to a too large value (in absolute terms), your lambda is probably too large\n",
    "constants[\"LAMBDA_JACCARD\"] = 500.0\n",
    "\n",
    "## SGD optimization settings\n",
    "## ...smaller learning rates require more iterations to converge but may yield better optimization processes\n",
    "## ...batch size can be increased for larger datasets to speed up convergence\n",
    "constants[\"LEARNING_RATE\"] = 0.004  # Learning rate for Adam optimizer\n",
    "constants[\"BATCH_SIZE\"] = 16  # Batch size for SGD optimizer\n",
    "\n",
    "\n",
    "# Properties of the desired solutions\n",
    "\n",
    "### N_CANDIDATE_SOLUTIONS = number of candidate solutions\n",
    "###                       = no. components of Gaussian mixture that approximate the posterior\n",
    "constants[\"N_CANDIDATE_SOLUTIONS\"] = 6\n",
    "constants[\"DESIRED_SPARSITY\"] = 8  # Expected # of features per solution\n",
    "### MIN_MU_THRESHOLD = minimum |μ| to consider a feature nonzero\n",
    "### ...adjust based on the scale of your features and the convergence behavior\n",
    "constants[\"MIN_MU_THRESHOLD\"] = 0.25\n",
    "\n",
    "\n",
    "# -------------------- NO NEED TO TOUCH CODE BELOW THIS CELL ---------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c01117",
   "metadata": {},
   "source": [
    "# Run the feature selector on your data\n",
    "\n",
    "There is no need to touch any code below this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34906baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = BayesianFeatureSelector(\n",
    "    n_features=constants[\"N_FEATURES\"],\n",
    "    n_components=constants[\"N_CANDIDATE_SOLUTIONS\"],\n",
    "    X=X,\n",
    "    y=y,\n",
    "    prior=constants[\"PRIOR_TYPE\"],\n",
    "    sss_sparsity=constants[\"PRIOR_SPARSITY\"],\n",
    "    var_slab=constants[\"VAR_SLAB\"],\n",
    "    var_spike=constants[\"VAR_SPIKE\"],\n",
    "    weight_slab=constants[\"WEIGHT_SLAB\"],\n",
    "    weight_spike=constants[\"WEIGHT_SPIKE\"],\n",
    "    student_df=constants[\"STUDENT_DF\"],\n",
    "    student_scale=constants[\"STUDENT_SCALE\"],\n",
    "    lr=constants[\"LEARNING_RATE\"],\n",
    "    batch_size=constants[\"BATCH_SIZE\"],\n",
    "    n_iter=constants[\"N_ITER\"],\n",
    ")\n",
    "\n",
    "history = selector.optimize(\n",
    "    regularize=constants[\"IS_REGULARIZED\"],\n",
    "    lambda_jaccard=constants[\"LAMBDA_JACCARD\"],\n",
    "    verbose=verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9c908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_search_history:\n",
    "    show_algorithm_progress(\n",
    "        history,\n",
    "        original_feature_names_mapping=feature_to_name,\n",
    "    )\n",
    "\n",
    "    show_final_alphas(\n",
    "        history,\n",
    "        show_bar_plot=False,\n",
    "        show_pie_chart=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21569d44",
   "metadata": {},
   "source": [
    "## Outlier Analysis\n",
    "\n",
    "The outlier analysis helps identify features with unusually high importance values (mu, either positive or negative) in each component.\n",
    "\n",
    "Ideally, the detected outliers are identical to the final solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff7858",
   "metadata": {},
   "outputs": [],
   "source": [
    "if outlier_analysis:\n",
    "    for outlier_threshold_coeff in [2.5, 3.0, 3.5]:\n",
    "        show_outlier_features_by_component(\n",
    "            history=history,\n",
    "            use_median=False,\n",
    "            outlier_threshold_coeff=outlier_threshold_coeff,\n",
    "            original_feature_names_mapping=feature_to_name,\n",
    "            use_markdown=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e216f",
   "metadata": {},
   "source": [
    "# Show the results\n",
    "\n",
    "## Overview of short solutions\n",
    "\n",
    "The short solutions are just the most important features from the long solutions. The number of features selected is defined by the desired sparsity parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fca799",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"### Required sparsity: {constants[\"DESIRED_SPARSITY\"]}\"))\n",
    "\n",
    "solutions, final_parameters, full_nonzero_solutions = recover_solutions(\n",
    "    search_history=history,\n",
    "    desired_sparsity=constants[\"DESIRED_SPARSITY\"],\n",
    "    min_mu_threshold=constants[\"MIN_MU_THRESHOLD\"],\n",
    "    verbose=verbose,\n",
    "    original_feature_names_mapping=feature_to_name,\n",
    ")\n",
    "\n",
    "show_unique_features(solutions, use_markdown=True)\n",
    "\n",
    "show_features_in_components(\n",
    "    solutions=solutions,\n",
    "    features_to_show=get_unique_features(solutions),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3067318f",
   "metadata": {},
   "source": [
    "## Overview of full (long) solutions\n",
    "\n",
    "The 'long' solutions are the actual solutions (full sets of features) found by the algorithm. Their sparsity may not be as strong as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8102165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_solution_summary(\n",
    "    solution_data=full_nonzero_solutions,\n",
    "    title=\"Full solutions found by the feature selector, ordered by importance\",\n",
    "    value_column=\"Feature\",\n",
    ")\n",
    "\n",
    "show_unique_features_from_full_solutions(full_nonzero_solutions)\n",
    "\n",
    "long_solutions = get_features_from_long_solutions(full_nonzero_solutions)\n",
    "show_features_in_components(\n",
    "    solutions=long_solutions,\n",
    "    features_to_show=get_unique_features(long_solutions),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8e4b7",
   "metadata": {},
   "source": [
    "## Regression for long solutions\n",
    "\n",
    "See how the long candidate solutions perform when simple logistic/linear regression is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab5b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_regressions_for_solutions:\n",
    "    show_regression_results_for_solutions(\n",
    "        solutions=get_features_from_long_solutions(solutions=full_nonzero_solutions),\n",
    "        df=df,\n",
    "        response=response,\n",
    "        use_standard_scaler=True,\n",
    "        penalty=\"l2\",\n",
    "        verbose=verbose,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c168e1e",
   "metadata": {},
   "source": [
    "# Performance diagnostics and recommendations (work in progress)\n",
    "\n",
    "Let us assess the feature selector's progress history to evaluate the reliability of the results. Based on the diagnostics, hyperparameter tuning might be recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fc5d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_diagnostics:\n",
    "    diagnostics = run_performance_diagnostics(\n",
    "        history,\n",
    "        desired_sparsity=constants[\"DESIRED_SPARSITY\"],\n",
    "        verbose=verbose_diagnostics,\n",
    "    )\n",
    "else:\n",
    "    display(Markdown(\"**Performance diagnostics and recommendations are disabled.**\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d13cc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_diagnostics and show_recommendations:\n",
    "    display_recommendations(diagnostics=diagnostics, constants=constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83803db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
