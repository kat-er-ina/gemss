{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of results\n",
    "\n",
    "This notebook demonstrates usage of the `tabpfn_evaluate` function for evaluating a dataset with TabPFN, outer cross-validation, and SHAP feature importances.\n",
    "\n",
    "Change the `task` variable and `X`, `y` inputs to use your own data if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotly import io as pio\n",
    "from gemss.utils.utils import load_feature_lists_json\n",
    "from gemss.postprocessing.result_postprocessing import get_unique_features\n",
    "from gemss.postprocessing.tabpfn_evaluation import tabpfn_evaluate\n",
    "\n",
    "pio.renderers.default = \"notebook_connected\"  # Ensures plotly plots show in notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9624ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original dataset\n",
    "dataset_name = \"shelflife_data_all.csv\"\n",
    "index_column_name = \"sample ID\"\n",
    "label_column_name = \"label\"\n",
    "\n",
    "# input data = output of GEMSS feature selection\n",
    "experiment_id = 1\n",
    "mydir = f\"./results/experiment_{experiment_id}\"\n",
    "features_filename = f\"{mydir}/all_candidate_solutions.json\"\n",
    "\n",
    "# output files for TabPFN evaluation results\n",
    "evaluation_output_filename = f\"{mydir}/tabpfn_evaluation_average_scores.csv\"\n",
    "importances_output_filename = f\"{mydir}/tabpfn_feature_importances.csv\"\n",
    "# delete old file if it exists\n",
    "if os.path.exists(evaluation_output_filename):\n",
    "    os.remove(evaluation_output_filename)\n",
    "if os.path.exists(importances_output_filename):\n",
    "    os.remove(importances_output_filename)\n",
    "\n",
    "\n",
    "# optionally select only numeric features\n",
    "# GEMSS selector works only with numeric types\n",
    "# but TabPFN can handle any type of data\n",
    "only_numeric_features = True\n",
    "\n",
    "# select the type of solutions to extract\n",
    "solution_type = \"Outlier features (STD_3.0)\"\n",
    "# solution_type = \"Top features\"\n",
    "# solution_type = \"Full features\"\n",
    "\n",
    "# evaluation settings:\n",
    "# - always evaluate using all features found by all candidate solutions of a given type\n",
    "#   ...good for a quick POC\n",
    "# - each component (= candidate solution) can be evaluated separately\n",
    "# - random baseline evaluates a model with randomly selected features of the same count as all unique features found in the selected solution\n",
    "evaluate_each_component = False\n",
    "compute_random_baseline = True\n",
    "\n",
    "compute_shapley_explanations = (\n",
    "    False  # can be costly; if True, n_folds must be at least 2\n",
    ")\n",
    "shap_sample_size = 50  # maximum number of samples for which to compute SHAP values (only if compute_shapley_explanations=True)\n",
    "\n",
    "n_folds = 2  # number of outer CV folds (tests on unseen data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8672db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the features file using utility loader\n",
    "all_features_lists = load_feature_lists_json(features_filename)[0]\n",
    "feature_dict_titles = list(all_features_lists.keys())\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"**Feature lists loaded from** `{features_filename}`: {len(feature_dict_titles)} types of solutions available\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_overview = pd.DataFrame(\n",
    "    index=[cname for cname in all_features_lists[feature_dict_titles[0]].keys()],\n",
    "    columns=feature_dict_titles,\n",
    ")\n",
    "for title, feature_dict in all_features_lists.items():\n",
    "    df_overview[title] = feature_dict\n",
    "\n",
    "for i in range(df_overview.shape[0]):\n",
    "    for j in range(df_overview.shape[1]):\n",
    "        df_overview.iat[i, j] = len(df_overview.iat[i, j])\n",
    "\n",
    "display(Markdown(\"**Numbers of features** in candidate solutions\"))\n",
    "display(df_overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4ef44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which type of solution to display\n",
    "solutions = all_features_lists[solution_type]\n",
    "\n",
    "# get the number of components available\n",
    "n_components = len(solutions)\n",
    "\n",
    "solutions_df = pd.DataFrame()\n",
    "max_length = max(len(clist) for clist in solutions.values())\n",
    "for cname, clist in solutions.items():\n",
    "    formatted_values = pd.Series(clist).apply(str)\n",
    "    padded_series = pd.Series([None] * max_length)\n",
    "    padded_series.iloc[: len(formatted_values)] = formatted_values.values\n",
    "    solutions_df[cname] = padded_series\n",
    "\n",
    "display(\n",
    "    Markdown(f\"### Experiment #{experiment_id}: {solution_type} candidate solutions\")\n",
    ")\n",
    "display(solutions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd659ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "df = pd.read_csv(f\"../data/{dataset_name}\", index_col=index_column_name)\n",
    "allowed_missing_percentage = 80  # 0 - 100\n",
    "\n",
    "# drop rows with missing labels\n",
    "df = df.dropna(subset=[label_column_name])\n",
    "\n",
    "# consider only numeric data\n",
    "if only_numeric_features:\n",
    "    df = df.select_dtypes([\"number\"]).astype(float)\n",
    "\n",
    "# separate response\n",
    "y = df.pop(label_column_name)\n",
    "\n",
    "# Get basic statistics about the dataset\n",
    "features_description = pd.DataFrame(\n",
    "    {\n",
    "        \"Valid Count\": df.count().astype(int),\n",
    "        \"Missing Count\": df.isnull().sum(),\n",
    "        \"Missing Percentage\": ((df.isnull().sum() / len(df)) * 100).astype(int),\n",
    "    }\n",
    ")\n",
    "\n",
    "# optional: exclude features with too many missing values\n",
    "nan_features = features_description[\n",
    "    features_description[\"Missing Percentage\"] >= allowed_missing_percentage\n",
    "].index\n",
    "df = df.drop(nan_features, axis=1)\n",
    "\n",
    "display(Markdown(\"**Dataset loaded:**\"))\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d5304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only features that will be used\n",
    "all_selected_features = get_unique_features(solutions)\n",
    "X_df = df[all_selected_features]\n",
    "\n",
    "# show info\n",
    "display(Markdown(\"**Selected features:**\"))\n",
    "display(X_df.describe())\n",
    "# X_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run TabPFN Evaluation with optional SHAP\n",
    "\n",
    "- Outer cross-validation\n",
    "- Feature scaling\n",
    "- SHAP explanations\n",
    "- Prints metrics for each fold\n",
    "\n",
    "> For large X, SHAP explanations may take time. For a quick demo, use a small subset or reduce folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "solutions[\"all_selected_features\"] = all_selected_features\n",
    "\n",
    "if evaluate_each_component:\n",
    "    for component, feature_list in solutions.items():\n",
    "        display(Markdown(f\"## {component.upper()}: {len(feature_list)} features\"))\n",
    "        results[component] = tabpfn_evaluate(\n",
    "            X_df[feature_list],\n",
    "            y,\n",
    "            apply_scaling=None,\n",
    "            outer_cv_folds=n_folds,\n",
    "            tabpfn_kwargs=None,\n",
    "            random_state=42,\n",
    "            verbose=True,\n",
    "            explain=compute_shapley_explanations,\n",
    "            shap_sample_size=shap_sample_size,\n",
    "        )\n",
    "        display(\n",
    "            Markdown(\n",
    "                \"----------------------------------------------------------------\\n\"\n",
    "            )\n",
    "        )\n",
    "else:\n",
    "    # compute using all discovered features\n",
    "    # quick validation to verify the information is contained in them\n",
    "    results[\"all_selected_features\"] = tabpfn_evaluate(\n",
    "        X_df[all_selected_features],\n",
    "        y,\n",
    "        apply_scaling=None,\n",
    "        outer_cv_folds=n_folds,\n",
    "        tabpfn_kwargs=None,\n",
    "        random_state=42,\n",
    "        verbose=True,\n",
    "        explain=compute_shapley_explanations,\n",
    "        shap_sample_size=shap_sample_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719b9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_random_baseline:\n",
    "    # get a random subset of features of the same size for comparison\n",
    "    # exclude the selected features\n",
    "    random_features = np.random.choice(\n",
    "        df.columns.difference(all_selected_features),\n",
    "        size=len(all_selected_features),\n",
    "        replace=False,\n",
    "    )\n",
    "    X_random = df[random_features]\n",
    "\n",
    "    results[f\"random_features_{len(random_features)}\"] = tabpfn_evaluate(\n",
    "        X_random,\n",
    "        y,\n",
    "        apply_scaling=None,\n",
    "        outer_cv_folds=n_folds,\n",
    "        tabpfn_kwargs=None,\n",
    "        random_state=42,\n",
    "        verbose=True,\n",
    "        explain=compute_shapley_explanations,\n",
    "        shap_sample_size=shap_sample_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Results: Average Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_average_scores = pd.DataFrame()\n",
    "\n",
    "for cname, cresults in results.items():\n",
    "    df_average_scores[cname] = pd.Series(cresults[\"average_scores\"])\n",
    "\n",
    "df_average_scores.to_csv(evaluation_output_filename)\n",
    "\n",
    "display(df_average_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances (SHAP, mean per fold)\n",
    "Each dictionary below shows mean absolute SHAP values for features in a CV fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cname, cresults in results.items():\n",
    "    if \"shap_explanations_per_fold\" in cresults.keys():\n",
    "        component_title = f\"{cname.upper()}: feature importances\"\n",
    "        display(Markdown(f\"## {component_title}\"))\n",
    "        with open(importances_output_filename, \"a\") as f:\n",
    "            print(component_title, file=f)\n",
    "            for fold, shap_imp in enumerate(\n",
    "                cresults.get(\"shap_explanations_per_fold\", [])\n",
    "            ):\n",
    "                fold_importances = pd.Series(shap_imp).sort_values(ascending=False)\n",
    "                fold_title = f\"Fold {fold+1} SHAP Feature Importances:\"\n",
    "                print(fold_importances, file=f)\n",
    "                display(Markdown(f\"### {fold_title}\"))\n",
    "                display(fold_importances)\n",
    "                display(\n",
    "                    Markdown(\n",
    "                        \"----------------------------------------------------------------\\n\"\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea19034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
