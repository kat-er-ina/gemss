{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEMSS Tier Results Analysis\n",
    "\n",
    "This notebook loads and analyzes the aggregated results from the tiered experiments. \n",
    "It reads the `tier_summary_metrics.csv` file generated by the experiment runner and provides visualizations to assess algorithm performance across different parameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "\n",
    "from gemss.diagnostics.experiment_results_visualizations import (\n",
    "    plot_solution_grouped,\n",
    "    plot_solution_comparison,\n",
    "    plot_si_asi_scatter,\n",
    "    analyze_metric_results,\n",
    "    plot_heatmap,\n",
    "    plot_metric_vs_hyperparam,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select tiers and load data\n",
    "\n",
    "Specify the Tier IDs you want to analyze. The code assumes your results are stored in `../scripts/results/tier{ID}/tier_summary_metrics.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tier_id_list = [3]\n",
    "# tier_id_list = [7]\n",
    "# tier_id_list = [1, 2, 3, 4]\n",
    "tier_id_list = [1, 2, 3, 4, 5, 6, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify metric columns (those containing the base name of coverage metrics)\n",
    "# List synchronized with keys returned by calculate_coverage_metrics in run_experiment.py\n",
    "# All coverage metrics are numeric (possibly None)\n",
    "COVERAGE_METRICS = [\n",
    "    \"Recall\",\n",
    "    \"Precision\",\n",
    "    \"F1_Score\",\n",
    "    \"Jaccard\",\n",
    "    \"Miss_Rate\",\n",
    "    \"FDR\",\n",
    "    \"Global_Miss_Rate\",\n",
    "    \"Global_FDR\",\n",
    "    \"Success_Index\",\n",
    "    \"Adjusted_Success_Index\",\n",
    "]\n",
    "\n",
    "SOLUTION_OPTIONS = [\n",
    "    \"full\",\n",
    "    \"top\",\n",
    "    \"outlier_STD_2.0\",\n",
    "    \"outlier_STD_2.5\",\n",
    "    \"outlier_STD_3.0\",\n",
    "]\n",
    "\n",
    "potential_params = [\n",
    "    \"N_SAMPLES\",\n",
    "    \"N_FEATURES\",\n",
    "    \"SAMPLE_VS_FEATURE_RATIO\",\n",
    "    \"SPARSITY\",\n",
    "    \"N_GENERATING_SOLUTIONS\",\n",
    "    \"N_CANDIDATE_SOLUTIONS\",\n",
    "    \"NOISE_STD\",\n",
    "    \"NAN_RATIO\",\n",
    "    \"LAMBDA_JACCARD\",\n",
    "    \"BINARY_RESPONSE_RATIO\",\n",
    "    \"BATCH_SIZE\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for tier_id in tier_id_list:\n",
    "\n",
    "    results_path = f\"../scripts/results/tier{tier_id}/tier_summary_metrics.csv\"\n",
    "\n",
    "    if os.path.exists(results_path):\n",
    "        df_tier = pd.read_csv(results_path)\n",
    "        print(\n",
    "            f\"Successfully loaded {len(df_tier)} experiment records from Tier {tier_id}.\"\n",
    "        )\n",
    "        # Ensure numeric columns are actually numeric\n",
    "        metric_cols = [\n",
    "            c for c in df_tier.columns if any(x in c for x in COVERAGE_METRICS)\n",
    "        ]\n",
    "        for col in metric_cols:\n",
    "            if col in df_tier.columns:\n",
    "                df_tier[col] = pd.to_numeric(df_tier[col], errors=\"coerce\")\n",
    "\n",
    "        # Add TIER_ID column\n",
    "        df_tier[\"TIER_ID\"] = int(tier_id)\n",
    "\n",
    "        # Add EXPERIMENT_ID column: {tier_id}.{experiment_number_in_tier}\n",
    "        df_tier[\"EXPERIMENT_ID\"] = str(tier_id) + \".\" + (df_tier.index + 1).astype(str)\n",
    "\n",
    "    else:\n",
    "        print(f\"ERROR: File not found at {results_path}\")\n",
    "        print(\"Please run the experiments for this tier first, or check the path.\")\n",
    "        df_tier = pd.DataFrame()\n",
    "\n",
    "    # Append to main DataFrame\n",
    "    df = pd.concat([df, df_tier], ignore_index=True)\n",
    "\n",
    "\n",
    "# Add the \"SAMPLE_VS_FEATURE_RATIO\" column\n",
    "df[\"SAMPLE_VS_FEATURE_RATIO\"] = df[\"N_SAMPLES\"] / df[\"N_FEATURES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"### All results for tiers: {tier_id_list}\"))\n",
    "display(df)\n",
    "\n",
    "display(Markdown(\"### Available Metrics\"))\n",
    "print(f\"Found {len(metric_cols)} metric columns.\")\n",
    "\n",
    "display(Markdown(\"### Varied Parameters\"))\n",
    "\n",
    "# Identify which parameters actually vary in this dataset\n",
    "varied_params = [p for p in potential_params if p in df.columns and df[p].nunique() > 1]\n",
    "unvaried_params = [\n",
    "    p for p in potential_params if p in df.columns and p not in varied_params\n",
    "]\n",
    "display(Markdown(f\"Parameters that vary in this tier:\\n {varied_params}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the df pivoted by solution type\n",
    "df_pivot_solution = pd.DataFrame()\n",
    "for solution in SOLUTION_OPTIONS:\n",
    "    solution_cols = [col for col in df.columns if solution in col]\n",
    "    df_solution = df[[\"TIER_ID\"] + varied_params + solution_cols].copy()\n",
    "    df_solution.rename(\n",
    "        columns={col: col.replace(f\"{solution}_\", \"\") for col in solution_cols},\n",
    "        inplace=True,\n",
    "    )\n",
    "    df_solution[\"solution_type\"] = solution\n",
    "    df_pivot_solution = pd.concat([df_pivot_solution, df_solution], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quick performance summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholds for performance evaluation:\n",
    "\n",
    "from gemss.diagnostics.experiment_results_visualizations import THRESHOLDS_FOR_METRIC\n",
    "\n",
    "df_thresholds = pd.DataFrame()\n",
    "for metric, thresholds in THRESHOLDS_FOR_METRIC.items():\n",
    "    if thresholds is not None:\n",
    "        df_thresholds[metric] = pd.Series(thresholds)\n",
    "\n",
    "display(Markdown(f\"#### Performance thresholds for selected metrics\"))\n",
    "display(df_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    analyze_metric_results,\n",
    "    df=fixed(df),\n",
    "    tier=widgets.SelectMultiple(\n",
    "        options=tier_id_list,\n",
    "        value=tier_id_list,\n",
    "        description=\"Tier:\",\n",
    "    ),\n",
    "    solution_type=widgets.Dropdown(\n",
    "        options=sorted(SOLUTION_OPTIONS),\n",
    "        value=\"outlier_STD_2.0\",\n",
    "        description=\"Solution:\",\n",
    "    ),\n",
    "    metric_name=widgets.Dropdown(\n",
    "        options=sorted([\"Recall\", \"Precision\", \"F1_Score\"]),\n",
    "        value=\"Recall\",\n",
    "        description=\"Metric:\",\n",
    "    ),\n",
    "    thresholds=fixed(None),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparison of solution types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### Average values for selected metrics\"))\n",
    "display(\n",
    "    df_pivot_solution[[\"TIER_ID\", \"solution_type\"] + COVERAGE_METRICS]\n",
    "    .groupby([\"TIER_ID\", \"solution_type\"])\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all solution types in one plot, explore how different experimental parameters affect the algorithm's performance.\n",
    "\n",
    "**Instructions:**\n",
    "1. Select the **Metric** (e.g., `Success_Index` or `Recall`).\n",
    "2. Select the **X-Axis** parameter (e.g., `N_SAMPLES` or `NOISE_STD`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    plot_solution_comparison,\n",
    "    df=fixed(df),\n",
    "    tier=widgets.SelectMultiple(\n",
    "        options=tier_id_list,\n",
    "        value=tier_id_list,\n",
    "        description=\"Tier:\",\n",
    "    ),\n",
    "    solution_types=fixed(SOLUTION_OPTIONS),\n",
    "    metric_name=widgets.Dropdown(\n",
    "        options=COVERAGE_METRICS, value=\"Success_Index\", description=\"Metric:\"\n",
    "    ),\n",
    "    x_axis=widgets.Dropdown(\n",
    "        options=varied_params,\n",
    "        value=\"N_FEATURES\" if \"N_FEATURES\" in varied_params else varied_params[0],\n",
    "        description=\"X-Axis:\",\n",
    "    ),\n",
    "    hover_params=fixed(varied_params + unvaried_params),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison with grouping\n",
    "\n",
    "Explore how different experimental parameters affect the algorithm's performance.\n",
    "\n",
    "**Instructions:**\n",
    "1. Select the **Solution Type** (e.g., `outlier_STD_2.5` is usually recommended for unknown sparsity).\n",
    "2. Select the **Metric** (e.g., `Success_Index` or `Recall`).\n",
    "3. Select the **X-Axis** parameter (e.g., `N_SAMPLES` or `NOISE_STD`).\n",
    "4. Select a **Grouping** parameter (color) to see interaction effects (e.g., `SPARSITY`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    plot_solution_grouped,\n",
    "    df=fixed(df),\n",
    "    tier=widgets.SelectMultiple(\n",
    "        options=tier_id_list,\n",
    "        value=tier_id_list,\n",
    "        description=\"Tier:\",\n",
    "    ),\n",
    "    solution_type=widgets.Dropdown(\n",
    "        options=SOLUTION_OPTIONS,\n",
    "        value=\"outlier_STD_2.5\",\n",
    "        description=\"Solution:\",\n",
    "    ),\n",
    "    metric_name=widgets.Dropdown(\n",
    "        options=COVERAGE_METRICS, value=\"Success_Index\", description=\"Metric:\"\n",
    "    ),\n",
    "    x_axis=widgets.Dropdown(\n",
    "        options=varied_params,\n",
    "        value=\"N_FEATURES\" if \"N_FEATURES\" in varied_params else varied_params[0],\n",
    "        description=\"X-Axis:\",\n",
    "    ),\n",
    "    color_by=widgets.Dropdown(\n",
    "        options=[\"None\"] + varied_params,\n",
    "        value=\"NAN_RATIO\" if \"NAN_RATIO\" in varied_params else \"None\",\n",
    "        description=\"Group By:\",\n",
    "    ),\n",
    "    hover_params=fixed(varied_params + unvaried_params),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Heatmap of a metric vs. 2 features\n",
    "\n",
    "Heatmap visualizations to compare a metric w.r.t. two parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    plot_heatmap,\n",
    "    df=fixed(df),\n",
    "    tier=widgets.SelectMultiple(\n",
    "        options=tier_id_list,\n",
    "        value=tier_id_list,\n",
    "        description=\"Tier:\",\n",
    "    ),\n",
    "    solution_type=widgets.Dropdown(\n",
    "        options=sorted(SOLUTION_OPTIONS),\n",
    "        value=\"outlier_STD_2.0\",\n",
    "        description=\"Solution:\",\n",
    "    ),\n",
    "    metric_name=widgets.Dropdown(\n",
    "        options=sorted(COVERAGE_METRICS),\n",
    "        value=\"Success_Index\",\n",
    "        description=\"Metric:\",\n",
    "    ),\n",
    "    x_axis=widgets.Dropdown(\n",
    "        options=varied_params + unvaried_params,\n",
    "        value=\"N_FEATURES\" if \"N_FEATURES\" in varied_params else varied_params[0],\n",
    "        description=\"X-Axis:\",\n",
    "    ),\n",
    "    y_axis=widgets.Dropdown(\n",
    "        options=varied_params,\n",
    "        value=\"SPARSITY\" if \"SPARSITY\" in varied_params else varied_params[0],\n",
    "        description=\"Y-Axis:\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Effect of hyperparameters\n",
    "\n",
    "Compare the results of varying hyperparameters ``LAMBDA_JACCARD`` and ``BATCH_SIZE`` within relevant tiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_list = [\"LAMBDA_JACCARD\", \"BATCH_SIZE\"]\n",
    "select_metrics = [\n",
    "    \"Recall\",\n",
    "    \"Precision\",\n",
    "    \"F1_Score\",\n",
    "    \"Jaccard\",\n",
    "]\n",
    "# select only those columns that contain one of the select_metrics\n",
    "select_metric_cols = [\n",
    "    col for col in metric_cols if any(m in col for m in select_metrics)\n",
    "]\n",
    "for hyperparam in hyperparam_list:\n",
    "    for tier in tier_id_list:\n",
    "        if df[df[\"TIER_ID\"] == tier][hyperparam].nunique() > 1:\n",
    "            df_grouped = (\n",
    "                df[df[\"TIER_ID\"] == tier].groupby(hyperparam)[select_metric_cols].mean()\n",
    "            )\n",
    "            display(Markdown(f\"### Effect of **{hyperparam}**: Tier {tier}\"))\n",
    "            display(df_grouped)\n",
    "\n",
    "            plot_metric_vs_hyperparam(\n",
    "                df_grouped=df_grouped,\n",
    "                hyperparam=hyperparam,\n",
    "                solution_options=SOLUTION_OPTIONS,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trade-off Analysis: Success Index (Quantity) vs. Adjusted SI (Quality)\n",
    "\n",
    "Assess the \"selectivity\" of your solution. High SI but Low ASI means the solution is noisy (selecting too many features). A perfect solution lies on the diagonal because ASI <= SI.\n",
    "\n",
    "### Success Index (SI)\n",
    "\n",
    "The Success Index rewards recall but normalizes it by the difficulty of the problem (sparsity). It is defined as:\n",
    "$$\n",
    "\\text{SI} = \\frac{\\text{Recall}}{\\text{Problem Sparsity}} = \\frac{f_{\\text{correct}} / p_{\\text{generating}}}{p_{\\text{generating}} / p} = \\frac{p \\times f_{\\text{correct}}}{p_{\\text{generating}}^2}\n",
    "$$\n",
    "\n",
    "- $f_{\\text{correct}}$: Number of correctly identified generating features (True Positives).\n",
    "- $p_{\\text{generating}}$: Total number of generating features (Ground Truth size).\n",
    "- $p$: Total number of features in the dataset (Search space size).\n",
    "\n",
    "#### Interpretation & Values\n",
    "\n",
    "The SI value scales linearly with the number of correctly identified features ($f_{\\text{correct}}$).\n",
    "**It effectively answers:** \"How many times better did we perform compared to random guessing in a sparse environment?\"\n",
    "\n",
    "**Maximum Value (Perfect Recall):** the inverse of the problem's sparsity ratio.\n",
    "**Takeaway:** A higher maximum achievable SI indicates a harder problem. Achieving a high SI in a high-dimensional problem is a strong signal of success.\n",
    "**Minimum Value:** When $f_{\\text{correct}} = 0$ (Recall = 0.0):$$\\text{SI} = 0$$\n",
    "\n",
    "#### Performance Thresholds (\"Good\" vs. \"Fail\")\n",
    "\n",
    "Because the raw value of SI depends heavily on the dimensionality $p$, absolute thresholds (e.g., \"SI > 5 is good\") are misleading across different experiments. Evaluate SI relative to the maximum possible SI for that specific experiment.\n",
    "\n",
    "**Good Performance:** Recall $\\ge 0.8$.\n",
    "\n",
    "$$\\text{SI} \\ge 0.8 \\times \\frac{p}{p_{\\text{generating}}}$$\n",
    "\n",
    "**Acceptable / Moderate:** Recall $\\approx 0.5$.\n",
    "\n",
    "$$\\text{SI} \\approx 0.5 \\times \\frac{p}{p_{\\text{generating}}}$$\n",
    "\n",
    "**Fail:** Recall $< 0.2$.\n",
    "\n",
    "$$\\text{SI} < 0.2 \\times \\frac{p}{p_{\\text{generating}}}$$\n",
    "\n",
    "\n",
    "\n",
    "### Adjusted Success Index (ASI)\n",
    "\n",
    "The Adjusted Success Index adds a penalty for precision. It prevents the algorithm from \"cheating\" the SI by simply selecting every single feature.\n",
    "$$\\text{ASI} = \\text{Precision} \\times \\text{SI} = \\frac{f_{\\text{correct}}}{f} \\times \\frac{p \\times f_{\\text{correct}}}{p_{\\text{generating}}^2}\n",
    "$$\n",
    "- $f$: Total number of features selected by the algorithm.\n",
    "\n",
    "#### Interpretation & Values\n",
    "\n",
    "ASI balances the difficulty of finding the needle in the haystack (SI) with the efficiency of the search (Precision).\n",
    "\n",
    "**Maximum Value (Perfect Recall & Precision):** When Recall = 1.0 AND Precision = 1.0 (perfect recovery of exactly the generating set):\n",
    "$$\\text{ASI}_{\\text{max}} = \\text{SI}_{\\text{max}} = \\frac{p}{p_{\\text{generating}}}$$\n",
    "\n",
    "**Impact of False Positives:** If the algorithm achieves perfect recall but selects too many extra features (poor precision), ASI drops significantly compared to SI.\n",
    "\n",
    "\n",
    "#### Performance Thresholds\n",
    "\n",
    "**Excellent (Perfect Recovery):** $\\text{ASI} \\approx \\text{SI}_{\\text{max}}$ (Precision $\\approx 1.0$).\n",
    "\n",
    "**Good (High Recall, Acceptable Noise):** $\\text{ASI} \\approx 0.5 \\times \\text{SI}_{\\text{max}}$ (e.g., Recall=1.0, Precision=0.5 OR Recall=0.8, Precision=0.6).\n",
    "\n",
    "**Fail (Noisy or Missed):** $\\text{ASI} < 0.1 \\times \\text{SI}_{\\text{max}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    plot_si_asi_scatter,\n",
    "    df=fixed(df),\n",
    "    tier=widgets.SelectMultiple(\n",
    "        options=tier_id_list,\n",
    "        value=tier_id_list,\n",
    "        description=\"Tier:\",\n",
    "    ),\n",
    "    solution_type=widgets.Dropdown(\n",
    "        options=sorted(SOLUTION_OPTIONS),\n",
    "        value=\"outlier_STD_2.0\",\n",
    "        description=\"Solution:\",\n",
    "    ),\n",
    "    color_by=widgets.Dropdown(\n",
    "        options=[\"None\"] + varied_params,\n",
    "        value=\"NOISE_STD\" if \"NOISE_STD\" in varied_params else \"None\",\n",
    "        description=\"Color By:\",\n",
    "    ),\n",
    "    hover_params=fixed(varied_params),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
