{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEMSS experiment evaluation per test case\n",
    "\n",
    "Experiments on artificial data are ordered in 7 tiers that cover even more test cases. Since no experiments were repeated, they must be combined across tiers to answer questions regarding the algorithm performance (e.g. How is the performance affected by dimension/missing data/noise? How do binary classification and regression compare?).\n",
    "Many test cases combine experiments from Tiers 2-7 with corresponding subsets of Tier 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from gemss.experiment_assessment.experiment_results_interactive import (\n",
    "    show_interactive_performance_overview,\n",
    "    show_interactive_solution_comparison,\n",
    "    show_interactive_comparison_with_grouping,\n",
    "    show_interactive_heatmap,\n",
    "    show_interactive_si_asi_comparison,\n",
    ")\n",
    "from gemss.experiment_assessment.experiment_results_visualizations import (\n",
    "    plot_metric_analysis_overview,\n",
    "    plot_solution_comparison,\n",
    "    plot_solution_grouped,\n",
    "    plot_heatmap,\n",
    ")\n",
    "from gemss.experiment_assessment.experiment_results_analysis import (\n",
    "    CATEGORY_ORDER,\n",
    "    COVERAGE_METRICS,\n",
    "    CORE_METRICS,\n",
    "    SOLUTION_OPTIONS,\n",
    "    ALL_PARAMETERS,\n",
    "    DEFAULT_METRIC,\n",
    "    DEFAULT_AGGREGATION_FUNC,\n",
    "    DEFAULT_SOLUTION,\n",
    "    get_all_experiment_results,\n",
    "    choose_best_solution_per_group,\n",
    "    filter_df_best_solutions,\n",
    "    get_average_metrics_per_group,\n",
    "    analyze_metric_results,\n",
    "    compute_performance_overview,\n",
    "    show_performance_overview,\n",
    ")\n",
    "\n",
    "from gemss.experiment_assessment.case_analysis import (\n",
    "    CASE_DESCRIPTION,\n",
    "    CASE_SET_RANGES,\n",
    "    SUMMARY_CASES,\n",
    "    COLORING_PARAM_PER_CASESET,\n",
    "    SYMBOL_PARAM_PER_CASESET,\n",
    "    COLORING_PARAM_PER_CASE,\n",
    "    SYMBOL_PARAM_PER_CASE,\n",
    "    get_df_cases,\n",
    "    concatenate_cases,\n",
    ")\n",
    "\n",
    "# pio.renderers.default = \"vscode\"  # Ensures plotly plots show in VSCode\n",
    "\n",
    "# For HTML export compatibility\n",
    "pio.renderers.default = \"notebook_connected\"  # Show plots in exported notebooks\n",
    "# pio.renderers.default = \"iframe\"  # Use this for HTML exports with embedded plots\n",
    "# pio.renderers.default = \"json\"    # Use this for completely static exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_all_experiment_results(verbose=False)\n",
    "df[\"TIER_ID\"] = df[\"TIER_ID\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign experiments to test cases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases = get_df_cases(df)\n",
    "df_all_cases = concatenate_cases(df_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cases.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive performance overview for all solution types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_interactive_performance_overview(\n",
    "    df_all_cases,\n",
    "    group_identifier=\"CASE_ID\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select solution type\n",
    "\n",
    "Find best solution types for each test case. However, proceed with \"top\" solutions for all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_solutions = choose_best_solution_per_group(\n",
    "    df_all_cases,\n",
    "    group_identifier=\"CASE_ID\",\n",
    "    metric=DEFAULT_METRIC,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually choose the \"top\" solution type for all cases\n",
    "chosen_solutions = {\n",
    "    case_id: DEFAULT_SOLUTION for case_id in df_all_cases[\"CASE_ID\"].unique()\n",
    "}\n",
    "\n",
    "df_all_cases_filtered = filter_df_best_solutions(\n",
    "    df_all_cases,\n",
    "    # best_solutions=best_solutions,\n",
    "    best_solutions=chosen_solutions,\n",
    "    group_identifier=\"CASE_ID\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of best solutions' performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance_overview = compute_performance_overview(\n",
    "    df_all_cases_filtered,\n",
    "    select_metrics=CORE_METRICS,\n",
    ")\n",
    "show_performance_overview(\n",
    "    df_performance_overview,\n",
    "    select_metrics=CORE_METRICS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of test cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_interactive_comparison_with_grouping(\n",
    "    df_all_cases_filtered,\n",
    "    group_identifier=\"CASE_ID\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_interactive_heatmap(\n",
    "    df_all_cases_filtered,\n",
    "    group_identifier=\"CASE_ID\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_case_set(case_set):\n",
    "    \"\"\"\n",
    "    Run analysis for all cases in a given case set.\n",
    "    \"\"\"\n",
    "    case_range = CASE_SET_RANGES[case_set]\n",
    "    display(\n",
    "        Markdown(\n",
    "            f\"## Analysis for case set **{case_set.upper()}** ({len(case_range)} test cases)\"\n",
    "        )\n",
    "    )\n",
    "    for i in case_range:\n",
    "        display(Markdown(f\"### Performance for **CASE_ID = {i}**\"))\n",
    "        display(Markdown(CASE_DESCRIPTION[i]))\n",
    "\n",
    "        coloring_param = COLORING_PARAM_PER_CASE[case_set]\n",
    "        symbol_param = SYMBOL_PARAM_PER_CASE[case_set]\n",
    "\n",
    "        # Quick performance overview\n",
    "        plot_metric_analysis_overview(\n",
    "            df_all_cases_filtered,\n",
    "            identifiers_list=[i],\n",
    "            group_identifier=\"CASE_ID\",\n",
    "            metric_name=DEFAULT_METRIC,\n",
    "            solution_type=\"all types\",\n",
    "            custom_title=f\"{DEFAULT_METRIC} performance for CASE_ID {i}\",\n",
    "        )\n",
    "        # Main performance plot\n",
    "        hover_params = [\n",
    "            param\n",
    "            for param in ALL_PARAMETERS\n",
    "            if (param in df_all_cases_filtered.columns)\n",
    "            and (df_all_cases_filtered[param].nunique() > 1)\n",
    "        ]\n",
    "        plot_solution_grouped(\n",
    "            df=df_all_cases_filtered.sort_values(by=\"EXPERIMENT_ID\"),\n",
    "            solution_type=best_solutions[f\"CASE_ID = {i}\"],\n",
    "            metric_name=DEFAULT_METRIC,\n",
    "            color_by=coloring_param,\n",
    "            symbol_by=symbol_param,\n",
    "            x_axis=\"EXPERIMENT_ID\",  # subject to change\n",
    "            group_identifier=\"CASE_ID\",\n",
    "            identifiers_list=[i],\n",
    "            hover_params=hover_params,\n",
    "        )\n",
    "\n",
    "        if i in SUMMARY_CASES:\n",
    "            # Additional heatmap plot to show performance map for all N_FEATURES and N_SAMPLES combinations\n",
    "            plot_heatmap(\n",
    "                df=df_all_cases_filtered,\n",
    "                solution_type=best_solutions[f\"CASE_ID = {i}\"],\n",
    "                x_axis=coloring_param,\n",
    "                y_axis=\"N_FEATURES\" if coloring_param != \"N_FEATURES\" else \"N_SAMPLES\",\n",
    "                metric_name=DEFAULT_METRIC,\n",
    "                group_identifier=\"CASE_ID\",\n",
    "                identifiers_list=[i],\n",
    "            )\n",
    "            # Additional plot to compare Adjusted Success Index\n",
    "            plot_solution_grouped(\n",
    "                df=df_all_cases_filtered.sort_values(by=\"EXPERIMENT_ID\"),\n",
    "                solution_type=best_solutions[f\"CASE_ID = {i}\"],\n",
    "                metric_name=\"Adjusted_Success_Index\",\n",
    "                color_by=coloring_param,\n",
    "                symbol_by=symbol_param,\n",
    "                x_axis=\"EXPERIMENT_ID\",  # subject to change\n",
    "                group_identifier=\"CASE_ID\",\n",
    "                identifiers_list=[i],\n",
    "                hover_params=hover_params,\n",
    "            )\n",
    "\n",
    "        # Compute mean and median metrics for cases grouped by the coloring_param\n",
    "        display(Markdown(f\"### Summary statistics for CASE_ID = {i}\"))\n",
    "        for agg_type in [\"median\", \"mean\"]:\n",
    "            averages = get_average_metrics_per_group(\n",
    "                df_all_cases_filtered[df_all_cases_filtered[\"CASE_ID\"] == i],\n",
    "                group_identifier=coloring_param,\n",
    "                aggregation_func=agg_type,\n",
    "            )\n",
    "            df_averages = pd.concat(averages.values(), keys=averages.keys())\n",
    "            display(\n",
    "                Markdown(\n",
    "                    f\"\\n- **{agg_type.capitalize()}** values of core metrics grouped by **{coloring_param}**:\"\n",
    "                )\n",
    "            )\n",
    "            display(df_averages)\n",
    "\n",
    "        display(Markdown(\"---\"))\n",
    "        display(Markdown(\"<br>\"))\n",
    "    display(Markdown(\"---\"))\n",
    "    display(Markdown(\"<br>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case_set in CASE_SET_RANGES.keys():\n",
    "    if case_set in [\"baseline\", \"scalability\", \"samplerich\"]:\n",
    "        analyze_case_set(case_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case_set in CASE_SET_RANGES.keys():\n",
    "    if case_set in [\"adversity\", \"unbalanced\"]:\n",
    "        analyze_case_set(case_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case_set in CASE_SET_RANGES.keys():\n",
    "    if case_set in [\"jaccard\"]:\n",
    "        analyze_case_set(case_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case_set in CASE_SET_RANGES.keys():\n",
    "    if case_set in [\"reg_baseline\", \"reg_scalability\", \"reg_adversity\"]:\n",
    "        analyze_case_set(case_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case_set in CASE_SET_RANGES.keys():\n",
    "    if case_set in [\"reg_vs_class\"]:\n",
    "        analyze_case_set(case_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
