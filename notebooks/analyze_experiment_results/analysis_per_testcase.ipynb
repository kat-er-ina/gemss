{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEMSS experiment evaluation per test case\n",
    "\n",
    "Experiments on artificial data are ordered in 7 tiers that cover even more test cases. Since no experiments were repeated, they must be combined across tiers to answer questions regarding the algorithm performance (e.g. How is the performance affected by dimension/missing data/noise? How do binary classification and regression compare?).\n",
    "Many test cases combine experiments from Tiers 2-7 with corresponding subsets of Tier 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "\n",
    "from gemss.experiment_assessment.experiment_results_interactive import (\n",
    "    show_interactive_performance_overview,\n",
    "    show_interactive_solution_comparison,\n",
    "    show_interactive_comparison_with_grouping,\n",
    "    show_interactive_heatmap,\n",
    "    show_interactive_si_asi_comparison,\n",
    ")\n",
    "\n",
    "from gemss.experiment_assessment.experiment_results_analysis import (\n",
    "    COVERAGE_METRICS,\n",
    "    SOLUTION_OPTIONS,\n",
    "    ALL_PARAMETERS,\n",
    "    DEFAULT_METRIC,\n",
    "    DEFAULT_AGGREGATION_FUNC,\n",
    "    get_all_experiment_results,\n",
    "    choose_best_solution_per_group,\n",
    "    filter_df_best_solutions,\n",
    ")\n",
    "\n",
    "from gemss.experiment_assessment.case_analysis import (\n",
    "    CASE_DESCRIPTION,\n",
    "    get_df_cases,\n",
    "    concatenate_cases,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_all_experiment_results(verbose=False)\n",
    "df[\"TIER_ID\"] = df[\"TIER_ID\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign experiments to the cases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases = get_df_cases(df)\n",
    "df_all_cases = concatenate_cases(df_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_solutions = choose_best_solution_per_group(\n",
    "    df_all_cases,\n",
    "    group_identifier=\"CASE_ID\",\n",
    "    metric=DEFAULT_METRIC,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "df_all_cases_filtered = filter_df_best_solutions(\n",
    "    df_all_cases,\n",
    "    best_solutions=best_solutions,\n",
    "    group_identifier=\"CASE_ID\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_interactive_performance_overview(\n",
    "    df_all_cases_filtered,\n",
    "    group_identifier=\"CASE_ID\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
