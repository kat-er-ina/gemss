{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEMSS tier results analysis: comparison of hyperparameters\n",
    "\n",
    "This notebook loads and analyzes the aggregated results from the tiered experiments. \n",
    "It reads the `tier_summary_metrics.csv` file generated by the experiment runner and analyzes algorithm performance across different hyperparameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "\n",
    "from gemss.diagnostics.experiment_results_visualizations import (\n",
    "    THRESHOLDS_FOR_METRIC,\n",
    "    plot_solution_grouped,\n",
    "    plot_solution_comparison,\n",
    "    analyze_metric_results,\n",
    "    plot_heatmap,\n",
    "    plot_metric_vs_hyperparam,\n",
    ")\n",
    "from gemss.diagnostics.experiment_results_analysis import (\n",
    "    COVERAGE_METRICS,\n",
    "    SOLUTION_OPTIONS,\n",
    "    ALL_PARAMETERS,\n",
    "    HYPERPARAMETERS,\n",
    "    load_experiment_results,\n",
    "    print_dataframe_overview,\n",
    "    pivot_df_by_solution_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select tiers and load data\n",
    "\n",
    "Specify the Tier IDs you want to analyze. The code assumes your results are stored in `../../scripts/results/tier{ID}/tier_summary_metrics.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Successfully loaded 18 experiment records from **Tier 1**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Successfully loaded 9 experiment records from **Tier 2**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Successfully loaded 14 experiment records from **Tier 3**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Successfully loaded 22 experiment records from **Tier 4**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Successfully loaded 12 experiment records from **Tier 5**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Successfully loaded 29 experiment records from **Tier 6**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Successfully loaded 14 experiment records from **Tier 7**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### All results for tiers: [1, 2, 3, 4, 5, 6, 7]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- **Total experiments:** 118"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- **5 solution types:** full, top, outlier_STD_2.0, outlier_STD_2.5, outlier_STD_3.0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- **10 available metrics:** Recall, Precision, F1_Score, Jaccard, Miss_Rate, FDR, Global_Miss_Rate, Global_FDR, Success_Index, Adjusted_Success_Index"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- **Total metrics columns:** 50"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "- **Varied parameters in this analysis:** N_SAMPLES, N_FEATURES, SAMPLE_VS_FEATURE_RATIO, SPARSITY, N_CANDIDATE_SOLUTIONS, NOISE_STD, NAN_RATIO, [NOISE_STD, NAN_RATIO] COMBINATION, LAMBDA_JACCARD, BINARY_RESPONSE_RATIO"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tier_id_list = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "df, metric_cols, varied_params, unvaried_params = load_experiment_results(tier_id_list)\n",
    "\n",
    "print_dataframe_overview(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the df pivoted by solution type\n",
    "df_pivot = pivot_df_by_solution_type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of hyperparameters\n",
    "\n",
    "Compare the results of varying hyperparameters ``LAMBDA_JACCARD`` and ``BATCH_SIZE`` within relevant tiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_list = [\n",
    "    \"LAMBDA_JACCARD\",\n",
    "    \"BATCH_SIZE\",\n",
    "    \"[NOISE_STD, NAN_RATIO] COMBINATION\",\n",
    "    \"NOISE_STD\",\n",
    "    \"NAN_RATIO\",\n",
    "    \"SAMPLE_VS_FEATURE_RATIO\",\n",
    "]\n",
    "select_metrics = [\n",
    "    \"Recall\",\n",
    "    \"Precision\",\n",
    "    \"F1_Score\",\n",
    "]\n",
    "# select only those columns that contain one of the select_metrics\n",
    "select_metric_cols = [\n",
    "    col for col in metric_cols if any(m in col for m in select_metrics)\n",
    "]\n",
    "for hyperparam in hyperparam_list:\n",
    "    for tier in tier_id_list:\n",
    "        if df[df[\"TIER_ID\"] == tier][hyperparam].nunique() > 1:\n",
    "            df_grouped = (\n",
    "                df[df[\"TIER_ID\"] == tier].groupby(hyperparam)[select_metric_cols].mean()\n",
    "            )\n",
    "            display(Markdown(f\"## **Tier {tier}**\"))\n",
    "            display(Markdown(f\"### Effect of **{hyperparam}**\"))\n",
    "            display(df_grouped)\n",
    "\n",
    "            plot_metric_vs_hyperparam(\n",
    "                df_grouped=df_grouped,\n",
    "                hyperparam=hyperparam,\n",
    "                solution_options=SOLUTION_OPTIONS,\n",
    "            )\n",
    "\n",
    "    # draw a horizontal line\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thresholds = pd.DataFrame()\n",
    "for metric, thresholds in THRESHOLDS_FOR_METRIC.items():\n",
    "    if thresholds is not None:\n",
    "        df_thresholds[metric] = pd.Series(thresholds)\n",
    "\n",
    "display(Markdown(f\"#### Performance thresholds for selected metrics\"))\n",
    "display(df_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    analyze_metric_results,\n",
    "    df=fixed(df),\n",
    "    tier=widgets.SelectMultiple(\n",
    "        options=tier_id_list,\n",
    "        value=tier_id_list,\n",
    "        description=\"Tier:\",\n",
    "    ),\n",
    "    solution_type=widgets.Dropdown(\n",
    "        options=sorted(SOLUTION_OPTIONS),\n",
    "        value=\"outlier_STD_2.0\",\n",
    "        description=\"Solution:\",\n",
    "    ),\n",
    "    metric_name=widgets.Dropdown(\n",
    "        options=sorted([\"Recall\", \"Precision\", \"F1_Score\"]),\n",
    "        value=\"Recall\",\n",
    "        description=\"Metric:\",\n",
    "    ),\n",
    "    thresholds=fixed(None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
