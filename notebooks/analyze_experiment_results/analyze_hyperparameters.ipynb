{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEMSS tier results analysis: comparison of hyperparameters\n",
    "\n",
    "This notebook loads and analyzes the aggregated results from the tiered experiments. \n",
    "It reads the `tier_summary_metrics.csv` file generated by the experiment runner and analyzes algorithm performance across different hyperparameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "\n",
    "from gemss.diagnostics.experiment_results_visualizations import (\n",
    "    THRESHOLDS_FOR_METRIC,\n",
    "    plot_solution_grouped,\n",
    "    plot_solution_comparison,\n",
    "    plot_si_asi_scatter,\n",
    "    analyze_metric_results,\n",
    "    plot_heatmap,\n",
    "    plot_metric_vs_hyperparam,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select tiers and load data\n",
    "\n",
    "Specify the Tier IDs you want to analyze. The code assumes your results are stored in `../scripts/results/tier{ID}/tier_summary_metrics.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tier_id_list = [3]\n",
    "# tier_id_list = [7]\n",
    "# tier_id_list = [1, 2, 3, 4]\n",
    "tier_id_list = [1, 2, 3, 4, 5, 6, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify metric columns (those containing the base name of coverage metrics)\n",
    "# List synchronized with keys returned by calculate_coverage_metrics in run_experiment.py\n",
    "# All coverage metrics are numeric (possibly None)\n",
    "COVERAGE_METRICS = [\n",
    "    \"Recall\",\n",
    "    \"Precision\",\n",
    "    \"F1_Score\",\n",
    "    \"Jaccard\",\n",
    "    \"Miss_Rate\",\n",
    "    \"FDR\",\n",
    "    \"Global_Miss_Rate\",\n",
    "    \"Global_FDR\",\n",
    "    \"Success_Index\",\n",
    "    \"Adjusted_Success_Index\",\n",
    "]\n",
    "\n",
    "SOLUTION_OPTIONS = [\n",
    "    \"full\",\n",
    "    \"top\",\n",
    "    \"outlier_STD_2.0\",\n",
    "    \"outlier_STD_2.5\",\n",
    "    \"outlier_STD_3.0\",\n",
    "]\n",
    "\n",
    "potential_params = [\n",
    "    \"N_SAMPLES\",\n",
    "    \"N_FEATURES\",\n",
    "    \"SAMPLE_VS_FEATURE_RATIO\",\n",
    "    \"SPARSITY\",\n",
    "    \"N_GENERATING_SOLUTIONS\",\n",
    "    \"N_CANDIDATE_SOLUTIONS\",\n",
    "    \"NOISE_STD\",\n",
    "    \"NAN_RATIO\",\n",
    "    \"[NOISE_STD, NAN_RATIO] COMBINATION\",\n",
    "    \"LAMBDA_JACCARD\",\n",
    "    \"BINARY_RESPONSE_RATIO\",\n",
    "    \"BATCH_SIZE\",\n",
    "    \"BINARIZE\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for tier_id in tier_id_list:\n",
    "\n",
    "    results_path = f\"../../scripts/results/tier{tier_id}/tier_summary_metrics.csv\"\n",
    "\n",
    "    if os.path.exists(results_path):\n",
    "        df_tier = pd.read_csv(results_path)\n",
    "        print(\n",
    "            f\"Successfully loaded {len(df_tier)} experiment records from Tier {tier_id}.\"\n",
    "        )\n",
    "        # Ensure numeric columns are actually numeric\n",
    "        metric_cols = [\n",
    "            c for c in df_tier.columns if any(x in c for x in COVERAGE_METRICS)\n",
    "        ]\n",
    "        for col in metric_cols:\n",
    "            if col in df_tier.columns:\n",
    "                df_tier[col] = pd.to_numeric(df_tier[col], errors=\"coerce\")\n",
    "\n",
    "        # Add TIER_ID column\n",
    "        df_tier[\"TIER_ID\"] = int(tier_id)\n",
    "\n",
    "        # Add EXPERIMENT_ID column: {tier_id}.{experiment_number_in_tier}\n",
    "        df_tier[\"EXPERIMENT_ID\"] = str(tier_id) + \".\" + (df_tier.index + 1).astype(str)\n",
    "\n",
    "    else:\n",
    "        print(f\"ERROR: File not found at {results_path}\")\n",
    "        print(\"Please run the experiments for this tier first, or check the path.\")\n",
    "        df_tier = pd.DataFrame()\n",
    "\n",
    "    # Append to main DataFrame\n",
    "    df = pd.concat([df, df_tier], ignore_index=True)\n",
    "\n",
    "\n",
    "# Add the \"SAMPLE_VS_FEATURE_RATIO\" column\n",
    "df[\"SAMPLE_VS_FEATURE_RATIO\"] = df[\"N_SAMPLES\"] / df[\"N_FEATURES\"]\n",
    "# Add a feature that combines information about noise and missingness\n",
    "df[\"[NOISE_STD, NAN_RATIO] COMBINATION\"] = (\n",
    "    \"[\" + df[\"NOISE_STD\"].astype(str) + \", \" + df[\"NAN_RATIO\"].astype(str) + \"]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"### All results for tiers: {tier_id_list}\"))\n",
    "# display(df)\n",
    "display(Markdown(f\"- **Total experiments:** {len(df)}\"))\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"- **{len(SOLUTION_OPTIONS)} solution types:** {', '.join(SOLUTION_OPTIONS)}\"\n",
    "    )\n",
    ")\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"- **{len(COVERAGE_METRICS)} available metrics:** {\", \".join(COVERAGE_METRICS)}\"\n",
    "    )\n",
    ")\n",
    "display(Markdown(f\"- **Total metrics columns:** {len(metric_cols)}\"))\n",
    "\n",
    "# Identify which parameters actually vary in this dataset\n",
    "varied_params = [p for p in potential_params if p in df.columns and df[p].nunique() > 1]\n",
    "unvaried_params = [\n",
    "    p for p in potential_params if p in df.columns and p not in varied_params\n",
    "]\n",
    "display(Markdown(f\"- **Varied Parameters:** {\", \".join(varied_params)}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the df pivoted by solution type\n",
    "df_pivot_solution = pd.DataFrame()\n",
    "for solution in SOLUTION_OPTIONS:\n",
    "    solution_cols = [col for col in df.columns if solution in col]\n",
    "    df_solution = df[[\"TIER_ID\"] + varied_params + solution_cols].copy()\n",
    "    df_solution[\"TIER_ID\"] = df_solution[\"TIER_ID\"].astype(str)\n",
    "    df_solution.rename(\n",
    "        columns={col: col.replace(f\"{solution}_\", \"\") for col in solution_cols},\n",
    "        inplace=True,\n",
    "    )\n",
    "    df_solution[\"solution_type\"] = solution\n",
    "    df_pivot_solution = pd.concat([df_pivot_solution, df_solution], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of hyperparameters\n",
    "\n",
    "Compare the results of varying hyperparameters ``LAMBDA_JACCARD`` and ``BATCH_SIZE`` within relevant tiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_list = [\n",
    "    \"LAMBDA_JACCARD\",\n",
    "    \"BATCH_SIZE\",\n",
    "    \"[NOISE_STD, NAN_RATIO] COMBINATION\",\n",
    "    \"NOISE_STD\",\n",
    "    \"NAN_RATIO\",\n",
    "    \"SAMPLE_VS_FEATURE_RATIO\",\n",
    "]\n",
    "select_metrics = [\n",
    "    \"Recall\",\n",
    "    \"Precision\",\n",
    "    \"F1_Score\",\n",
    "]\n",
    "# select only those columns that contain one of the select_metrics\n",
    "select_metric_cols = [\n",
    "    col for col in metric_cols if any(m in col for m in select_metrics)\n",
    "]\n",
    "for hyperparam in hyperparam_list:\n",
    "    for tier in tier_id_list:\n",
    "        if df[df[\"TIER_ID\"] == tier][hyperparam].nunique() > 1:\n",
    "            df_grouped = (\n",
    "                df[df[\"TIER_ID\"] == tier].groupby(hyperparam)[select_metric_cols].mean()\n",
    "            )\n",
    "            display(Markdown(f\"## **Tier {tier}**\"))\n",
    "            display(Markdown(f\"### Effect of **{hyperparam}**\"))\n",
    "            display(df_grouped)\n",
    "\n",
    "            plot_metric_vs_hyperparam(\n",
    "                df_grouped=df_grouped,\n",
    "                hyperparam=hyperparam,\n",
    "                solution_options=SOLUTION_OPTIONS,\n",
    "            )\n",
    "\n",
    "    # draw a horizontal line\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thresholds = pd.DataFrame()\n",
    "for metric, thresholds in THRESHOLDS_FOR_METRIC.items():\n",
    "    if thresholds is not None:\n",
    "        df_thresholds[metric] = pd.Series(thresholds)\n",
    "\n",
    "display(Markdown(f\"#### Performance thresholds for selected metrics\"))\n",
    "display(df_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    analyze_metric_results,\n",
    "    df=fixed(df),\n",
    "    tier=widgets.SelectMultiple(\n",
    "        options=tier_id_list,\n",
    "        value=tier_id_list,\n",
    "        description=\"Tier:\",\n",
    "    ),\n",
    "    solution_type=widgets.Dropdown(\n",
    "        options=sorted(SOLUTION_OPTIONS),\n",
    "        value=\"outlier_STD_2.0\",\n",
    "        description=\"Solution:\",\n",
    "    ),\n",
    "    metric_name=widgets.Dropdown(\n",
    "        options=sorted([\"Recall\", \"Precision\", \"F1_Score\"]),\n",
    "        value=\"Recall\",\n",
    "        description=\"Metric:\",\n",
    "    ),\n",
    "    thresholds=fixed(None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
