{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEMSS tier results analysis: comparison of hyperparameters\n",
    "\n",
    "This notebook loads and analyzes the aggregated results from the tiered experiments. \n",
    "It reads the `tier_summary_metrics.csv` file generated by the experiment runner and analyzes algorithm performance across different hyperparameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "\n",
    "from gemss.experiment_assessment.experiment_results_visualizations import (\n",
    "    THRESHOLDS_FOR_METRIC,\n",
    "    analyze_metric_results,\n",
    "    plot_metric_vs_hyperparam,\n",
    ")\n",
    "from gemss.experiment_assessment.experiment_results_analysis import (\n",
    "    COVERAGE_METRICS,\n",
    "    SOLUTION_OPTIONS,\n",
    "    ALL_PARAMETERS,\n",
    "    load_experiment_results,\n",
    "    print_dataframe_overview,\n",
    "    pivot_df_by_solution_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select tiers and load data\n",
    "\n",
    "Specify the Tier IDs you want to analyze. The code assumes your results are stored in `../../scripts/results/tier{ID}/tier_summary_metrics.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tier_id_list = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "df, metric_cols = load_experiment_results(tier_id_list)\n",
    "\n",
    "print_dataframe_overview(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the df pivoted by solution type\n",
    "df_pivot = pivot_df_by_solution_type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of hyperparameters\n",
    "\n",
    "Compare the results of varying hyperparameters ``LAMBDA_JACCARD`` and ``BATCH_SIZE`` within relevant tiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_list = [\n",
    "    \"LAMBDA_JACCARD\",\n",
    "    \"BATCH_SIZE\",\n",
    "    \"[NOISE_STD, NAN_RATIO] COMBINATION\",\n",
    "    \"NOISE_STD\",\n",
    "    \"NAN_RATIO\",\n",
    "    \"SAMPLE_VS_FEATURE_RATIO\",\n",
    "    \"BINARIZE\",\n",
    "]\n",
    "select_metrics = [\n",
    "    \"Recall\",\n",
    "    \"Precision\",\n",
    "    \"F1_Score\",\n",
    "]\n",
    "# select only those columns that contain one of the select_metrics\n",
    "select_metric_cols = [\n",
    "    col for col in metric_cols if any(m in col for m in select_metrics)\n",
    "]\n",
    "for hyperparam in hyperparam_list:\n",
    "    for tier in tier_id_list:\n",
    "        if df[df[\"TIER_ID\"] == tier][hyperparam].nunique() > 1:\n",
    "            df_grouped = (\n",
    "                df[df[\"TIER_ID\"] == tier].groupby(hyperparam)[select_metric_cols].mean()\n",
    "            )\n",
    "            display(Markdown(f\"## **Tier {tier}**\"))\n",
    "            display(Markdown(f\"### Effect of **{hyperparam}**\"))\n",
    "            display(df_grouped)\n",
    "\n",
    "            plot_metric_vs_hyperparam(\n",
    "                df_grouped=df_grouped,\n",
    "                hyperparam=hyperparam,\n",
    "                solution_options=SOLUTION_OPTIONS,\n",
    "            )\n",
    "\n",
    "    # draw a horizontal line\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_thresholds = pd.DataFrame()\n",
    "for metric, thresholds in THRESHOLDS_FOR_METRIC.items():\n",
    "    if thresholds is not None:\n",
    "        df_thresholds[metric] = pd.Series(thresholds)\n",
    "\n",
    "display(Markdown(f\"#### Performance thresholds for selected metrics\"))\n",
    "display(df_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    analyze_metric_results,\n",
    "    df=fixed(df),\n",
    "    tier=widgets.SelectMultiple(\n",
    "        options=tier_id_list,\n",
    "        value=tier_id_list,\n",
    "        description=\"Tier:\",\n",
    "    ),\n",
    "    solution_type=widgets.Dropdown(\n",
    "        options=sorted(SOLUTION_OPTIONS),\n",
    "        value=\"outlier_STD_2.0\",\n",
    "        description=\"Solution:\",\n",
    "    ),\n",
    "    metric_name=widgets.Dropdown(\n",
    "        options=sorted([\"Recall\", \"Precision\", \"F1_Score\"]),\n",
    "        value=\"Recall\",\n",
    "        description=\"Metric:\",\n",
    "    ),\n",
    "    thresholds=fixed(None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
