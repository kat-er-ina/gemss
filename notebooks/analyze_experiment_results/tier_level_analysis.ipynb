{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEMSS tier results analysis: experiment evaluation\n",
    "\n",
    "This notebook loads and analyzes the aggregated results from the tiered experiments. \n",
    "It reads the `tier_summary_metrics.csv` file generated by the experiment runner and provides visualizations to assess algorithm performance across different parameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gemss.experiment_assessment.experiment_results_interactive import (\n",
    "    show_interactive_performance_overview,\n",
    "    show_interactive_solution_comparison,\n",
    "    show_interactive_comparison_with_grouping,\n",
    "    show_interactive_heatmap,\n",
    "    show_interactive_si_asi_comparison,\n",
    ")\n",
    "from gemss.experiment_assessment.experiment_results_analysis import (\n",
    "    DEFAULT_AGGREGATION_FUNC,\n",
    "    DEFAULT_METRIC,\n",
    "    get_all_experiment_results,\n",
    "    choose_best_solution_per_group,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select tiers and load data\n",
    "\n",
    "Specify the Tier IDs you want to analyze. The code assumes your results are stored in `../../scripts/results/tier{ID}/tier_summary_metrics.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tier_id_list = [1, 2, 3, 4, 5, 6, 7]\n",
    "df = get_all_experiment_results(tier_id_list, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quick performance summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_interactive_performance_overview(\n",
    "    df,\n",
    "    group_identifier=\"TIER_ID\",\n",
    "    show_metric_thresholds=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparison of solution types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_solutions_per_tier = choose_best_solution_per_group(\n",
    "    df,\n",
    "    group_identifier=\"TIER_ID\",\n",
    "    metric=DEFAULT_METRIC,\n",
    "    aggregation_func=DEFAULT_AGGREGATION_FUNC,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all solution types in one plot, explore how different experimental parameters affect the algorithm's performance.\n",
    "\n",
    "**Instructions:**\n",
    "1. Select the **Metric** (e.g., `Success_Index` or `Recall`).\n",
    "2. Select the **X-Axis** parameter (e.g., `N_SAMPLES` or `NOISE_STD`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_interactive_solution_comparison(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparison with grouping\n",
    "\n",
    "Explore how different experimental parameters affect the algorithm's performance.\n",
    "\n",
    "**Instructions:**\n",
    "1. Select the **Solution Type** (e.g., `outlier_STD_2.5` is usually recommended for unknown sparsity).\n",
    "2. Select the **Metric** (e.g., `Success_Index` or `Recall`).\n",
    "3. Select the **X-Axis** parameter (e.g., `N_SAMPLES` or `NOISE_STD`).\n",
    "4. Select a **Grouping** parameter (color) to see interaction effects (e.g., `SPARSITY`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_interactive_comparison_with_grouping(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Heatmap of a metric vs. 2 features\n",
    "\n",
    "Heatmap visualizations to compare a metric w.r.t. two parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_interactive_heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trade-off Analysis: Success Index (Quantity) vs. Adjusted SI (Quality)\n",
    "\n",
    "Assess the \"selectivity\" of your solution. High SI but Low ASI means the solution is noisy (selecting too many features). A perfect solution lies on the diagonal because ASI <= SI.\n",
    "\n",
    "### Success Index (SI)\n",
    "\n",
    "The Success Index rewards recall but normalizes it by the difficulty of the problem (sparsity). It is defined as:\n",
    "$$\n",
    "\\text{SI} = \\frac{\\text{Recall}}{\\text{Problem Sparsity}} = \\frac{f_{\\text{correct}} / p_{\\text{generating}}}{p_{\\text{generating}} / p} = \\frac{p \\times f_{\\text{correct}}}{p_{\\text{generating}}^2}\n",
    "$$\n",
    "\n",
    "- $f_{\\text{correct}}$: Number of correctly identified generating features (True Positives).\n",
    "- $p_{\\text{generating}}$: Total number of generating features (Ground Truth size).\n",
    "- $p$: Total number of features in the dataset (Search space size).\n",
    "\n",
    "#### Interpretation & Values\n",
    "\n",
    "The SI value scales linearly with the number of correctly identified features ($f_{\\text{correct}}$).\n",
    "**It effectively answers:** \"How many times better did we perform compared to random guessing in a sparse environment?\"\n",
    "\n",
    "**Maximum Value (Perfect Recall):** the inverse of the problem's sparsity ratio.\n",
    "**Takeaway:** A higher maximum achievable SI indicates a harder problem. Achieving a high SI in a high-dimensional problem is a strong signal of success.\n",
    "**Minimum Value:** When $f_{\\text{correct}} = 0$ (i.e. $\\text{Recall} = 0.0$), it is $\\text{SI} = 0$.\n",
    "\n",
    "#### Performance Thresholds (\"Good\" vs. \"Fail\")\n",
    "\n",
    "Because the raw value of SI depends heavily on the dimensionality $p$, absolute thresholds (e.g., \"SI > 5 is good\") are misleading across different experiments. Evaluate SI relative to the maximum possible SI for that specific experiment.\n",
    "\n",
    "### Adjusted Success Index (ASI)\n",
    "\n",
    "The Adjusted Success Index adds a penalty for precision. It prevents the algorithm from \"cheating\" the SI by simply selecting every single feature.\n",
    "$$\\text{ASI} = \\text{Precision} \\times \\text{SI} = \\frac{f_{\\text{correct}}}{f} \\times \\frac{p \\times f_{\\text{correct}}}{p_{\\text{generating}}^2}\n",
    "$$\n",
    "- $f$: Total number of features selected by the algorithm.\n",
    "\n",
    "#### Interpretation & Values\n",
    "\n",
    "ASI balances the difficulty of finding the needle in the haystack (SI) with the efficiency of the search (Precision).\n",
    "\n",
    "**Maximum Value (Perfect Recall & Precision):** When Recall = 1.0 AND Precision = 1.0 (perfect recovery of exactly the generating set):\n",
    "$$\\text{ASI}_{\\text{max}} = \\text{SI}_{\\text{max}} = \\frac{p}{p_{\\text{generating}}}$$\n",
    "\n",
    "**Impact of False Positives:** If the algorithm achieves perfect recall but selects too many extra features (poor precision), ASI drops significantly compared to SI.\n",
    "\n",
    "\n",
    "#### Performance Thresholds\n",
    "\n",
    "**Excellent (Perfect Recovery):** $\\text{ASI} \\approx \\text{SI}_{\\text{max}}$ (Precision $\\approx 1.0$).\n",
    "\n",
    "**Good (High Recall, Acceptable Noise):** $\\text{ASI} \\approx 0.5 \\times \\text{SI}_{\\text{max}}$ (e.g., Recall=1.0, Precision=0.5 OR Recall=0.8, Precision=0.6).\n",
    "\n",
    "**Fail (Noisy or Missed):** $\\text{ASI} < 0.1 \\times \\text{SI}_{\\text{max}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_interactive_si_asi_comparison(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
